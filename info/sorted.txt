Namespace(backend='vllm', dataset='/home/jiangjz/llm/TDoppeladler/dataset/ShareGPT_V3_unfiltered_cleaned_split.json', dtype='auto', enforce_eager=False, hf_max_batch_size=None, input_len=None, max_model_len=None, model='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', n=1, num_prompts=1000, output_len=None, quantization=None, seed=0, tensor_parallel_size=1, tokenizer='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', trust_remote_code=False, use_beam_search=False)
ModelConfig hidden_size=4096, head_size=128, swap_space_bytes=32, num_kv_heads=32        num_layers=32
CacheConfig block_size=16, gpu_memory_utilization=0.9, swap_space_bytes=4294967296,         sliding_window=None
SchedulerConfig max_num_batched_tokens=4096, max_model_len=4096, max_num_seqs=256,         max_paddings=256
INFO 12-27 14:20:27 llm_engine.py:74] Initializing an LLM engine with config: model='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', tokenizer='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
input_metadata = InputMetadata(prompt_lens=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16], num_prompts=256, max_context_len=None,)
INFO 12-27 14:20:33 llm_engine.py:228] # GPU blocks: 953, # CPU blocks: 512
INFO 12-27 14:20:34 model_runner.py:403] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-27 14:20:34 model_runner.py:407] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
INFO 12-27 14:20:36 model_runner.py:449] Graph capturing finished in 2 secs.
promt select exit padding exceed_paddings=301, cur_paddings=210, exceeed_seq=11
total_padding=210
input_metadata = InputMetadata(prompt_lens=[4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], num_prompts=91, max_context_len=None,)
INFO 12-27 14:20:36 llm_engine.py:666] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 91 reqs, Swapped: 0 reqs, Pending: 909 reqs, GPU KV cache usage: 9.5%, CPU KV cache usage: 0.0%
promt select exit padding exceed_paddings=279, cur_paddings=182, exceeed_seq=16
total_padding=392
input_metadata = InputMetadata(prompt_lens=[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15], num_prompts=97, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21], num_prompts=68, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[22, 22], num_prompts=2, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[23], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[23, 23, 23], num_prompts=3, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[23, 23], num_prompts=2, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[25], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
total_padding=594
input_metadata = InputMetadata(prompt_lens=[59], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[66], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[92], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[107], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[105], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[120], num_prompts=1, max_context_len=None,)
INFO 12-27 14:20:41 llm_engine.py:666] Avg prompt throughput: 780.2 tokens/s, Avg generation throughput: 4666.1 tokens/s, Running: 119 reqs, Swapped: 0 reqs, Pending: 825 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 0.0%
total_padding=594
input_metadata = InputMetadata(prompt_lens=[128], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[118], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[119], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[161], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[178], num_prompts=1, max_context_len=None,)
total_padding=594
input_metadata = InputMetadata(prompt_lens=[180], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[196, 187], num_prompts=2, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[188], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[222], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[223], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[229], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[238], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[249], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[255], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[257], num_prompts=1, max_context_len=None,)
INFO 12-27 14:20:46 llm_engine.py:666] Avg prompt throughput: 607.1 tokens/s, Avg generation throughput: 2402.7 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 857 reqs, GPU KV cache usage: 97.2%, CPU KV cache usage: 0.0%
total_padding=603
input_metadata = InputMetadata(prompt_lens=[262], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[267], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[283], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[270], num_prompts=1, max_context_len=None,)
total_padding=603
input_metadata = InputMetadata(prompt_lens=[277], num_prompts=1, max_context_len=None,)
total_padding=614
input_metadata = InputMetadata(prompt_lens=[232, 221], num_prompts=2, max_context_len=None,)
total_padding=614
input_metadata = InputMetadata(prompt_lens=[216], num_prompts=1, max_context_len=None,)
total_padding=643
input_metadata = InputMetadata(prompt_lens=[207, 194, 191], num_prompts=3, max_context_len=None,)
total_padding=643
input_metadata = InputMetadata(prompt_lens=[193], num_prompts=1, max_context_len=None,)
total_padding=685
input_metadata = InputMetadata(prompt_lens=[197, 182, 170], num_prompts=3, max_context_len=None,)
total_padding=691
input_metadata = InputMetadata(prompt_lens=[167, 161], num_prompts=2, max_context_len=None,)
total_padding=691
input_metadata = InputMetadata(prompt_lens=[164], num_prompts=1, max_context_len=None,)
total_padding=693
input_metadata = InputMetadata(prompt_lens=[165, 163], num_prompts=2, max_context_len=None,)
total_padding=703
input_metadata = InputMetadata(prompt_lens=[165, 155], num_prompts=2, max_context_len=None,)
total_padding=703
input_metadata = InputMetadata(prompt_lens=[157], num_prompts=1, max_context_len=None,)
total_padding=711
input_metadata = InputMetadata(prompt_lens=[155, 147], num_prompts=2, max_context_len=None,)
total_padding=733
input_metadata = InputMetadata(prompt_lens=[143, 121], num_prompts=2, max_context_len=None,)
total_padding=761
input_metadata = InputMetadata(prompt_lens=[152, 124], num_prompts=2, max_context_len=None,)
total_padding=761
input_metadata = InputMetadata(prompt_lens=[123], num_prompts=1, max_context_len=None,)
total_padding=791
input_metadata = InputMetadata(prompt_lens=[164, 134], num_prompts=2, max_context_len=None,)
total_padding=804
input_metadata = InputMetadata(prompt_lens=[126, 139], num_prompts=2, max_context_len=None,)
total_padding=837
input_metadata = InputMetadata(prompt_lens=[177, 144], num_prompts=2, max_context_len=None,)
total_padding=837
input_metadata = InputMetadata(prompt_lens=[132], num_prompts=1, max_context_len=None,)
total_padding=841
input_metadata = InputMetadata(prompt_lens=[142, 138], num_prompts=2, max_context_len=None,)
total_padding=848
input_metadata = InputMetadata(prompt_lens=[131, 124], num_prompts=2, max_context_len=None,)
total_padding=848
input_metadata = InputMetadata(prompt_lens=[130], num_prompts=1, max_context_len=None,)
total_padding=848
input_metadata = InputMetadata(prompt_lens=[135], num_prompts=1, max_context_len=None,)
total_padding=939
input_metadata = InputMetadata(prompt_lens=[140, 129, 121, 124, 119, 116], num_prompts=6, max_context_len=None,)
total_padding=939
input_metadata = InputMetadata(prompt_lens=[115], num_prompts=1, max_context_len=None,)
total_padding=942
input_metadata = InputMetadata(prompt_lens=[124, 121], num_prompts=2, max_context_len=None,)
total_padding=955
input_metadata = InputMetadata(prompt_lens=[131, 125, 124], num_prompts=3, max_context_len=None,)
INFO 12-27 14:20:51 llm_engine.py:666] Avg prompt throughput: 1792.1 tokens/s, Avg generation throughput: 1319.7 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 822 reqs, GPU KV cache usage: 99.1%, CPU KV cache usage: 0.0%
total_padding=955
input_metadata = InputMetadata(prompt_lens=[161], num_prompts=1, max_context_len=None,)
total_padding=967
input_metadata = InputMetadata(prompt_lens=[151, 139], num_prompts=2, max_context_len=None,)
total_padding=999
input_metadata = InputMetadata(prompt_lens=[146, 131, 129], num_prompts=3, max_context_len=None,)
total_padding=1005
input_metadata = InputMetadata(prompt_lens=[122, 116], num_prompts=2, max_context_len=None,)
total_padding=1013
input_metadata = InputMetadata(prompt_lens=[128, 120], num_prompts=2, max_context_len=None,)
total_padding=1057
input_metadata = InputMetadata(prompt_lens=[116, 113, 119, 117, 112, 108, 104], num_prompts=7, max_context_len=None,)
total_padding=1070
input_metadata = InputMetadata(prompt_lens=[133, 130, 123], num_prompts=3, max_context_len=None,)
total_padding=1085
input_metadata = InputMetadata(prompt_lens=[110, 99, 106], num_prompts=3, max_context_len=None,)
total_padding=1114
input_metadata = InputMetadata(prompt_lens=[107, 95, 95, 102], num_prompts=4, max_context_len=None,)
total_padding=1114
input_metadata = InputMetadata(prompt_lens=[95], num_prompts=1, max_context_len=None,)
total_padding=1114
input_metadata = InputMetadata(prompt_lens=[98], num_prompts=1, max_context_len=None,)
total_padding=1140
input_metadata = InputMetadata(prompt_lens=[95, 87, 101, 95], num_prompts=4, max_context_len=None,)
total_padding=1149
input_metadata = InputMetadata(prompt_lens=[106, 97], num_prompts=2, max_context_len=None,)
total_padding=1160
input_metadata = InputMetadata(prompt_lens=[94, 99, 93], num_prompts=3, max_context_len=None,)
total_padding=1160
input_metadata = InputMetadata(prompt_lens=[118], num_prompts=1, max_context_len=None,)
total_padding=1183
input_metadata = InputMetadata(prompt_lens=[107, 119, 108], num_prompts=3, max_context_len=None,)
total_padding=1250
input_metadata = InputMetadata(prompt_lens=[114, 125, 112, 102, 105], num_prompts=5, max_context_len=None,)
total_padding=1250
input_metadata = InputMetadata(prompt_lens=[110], num_prompts=1, max_context_len=None,)
total_padding=1266
input_metadata = InputMetadata(prompt_lens=[114, 98], num_prompts=2, max_context_len=None,)
total_padding=1314
input_metadata = InputMetadata(prompt_lens=[120, 120, 102, 90], num_prompts=4, max_context_len=None,)
total_padding=1314
input_metadata = InputMetadata(prompt_lens=[77], num_prompts=1, max_context_len=None,)
total_padding=1318
input_metadata = InputMetadata(prompt_lens=[136, 132], num_prompts=2, max_context_len=None,)
INFO 12-27 14:20:56 llm_engine.py:666] Avg prompt throughput: 1450.6 tokens/s, Avg generation throughput: 1391.2 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 802 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 0.0%
total_padding=1353
input_metadata = InputMetadata(prompt_lens=[166, 173, 145], num_prompts=3, max_context_len=None,)
total_padding=1378
input_metadata = InputMetadata(prompt_lens=[138, 113], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=362, cur_paddings=249, exceeed_seq=80
total_padding=1627
input_metadata = InputMetadata(prompt_lens=[193, 158, 147, 119, 99], num_prompts=5, max_context_len=None,)
total_padding=1653
input_metadata = InputMetadata(prompt_lens=[80, 95, 84], num_prompts=3, max_context_len=None,)
total_padding=1655
input_metadata = InputMetadata(prompt_lens=[81, 83], num_prompts=2, max_context_len=None,)
total_padding=1655
input_metadata = InputMetadata(prompt_lens=[109], num_prompts=1, max_context_len=None,)
total_padding=1655
input_metadata = InputMetadata(prompt_lens=[204], num_prompts=1, max_context_len=None,)
total_padding=1686
input_metadata = InputMetadata(prompt_lens=[180, 149], num_prompts=2, max_context_len=None,)
total_padding=1769
input_metadata = InputMetadata(prompt_lens=[153, 122, 101], num_prompts=3, max_context_len=None,)
total_padding=1889
input_metadata = InputMetadata(prompt_lens=[278, 236, 200], num_prompts=3, max_context_len=None,)
total_padding=1889
input_metadata = InputMetadata(prompt_lens=[249], num_prompts=1, max_context_len=None,)
total_padding=1931
input_metadata = InputMetadata(prompt_lens=[208, 166], num_prompts=2, max_context_len=None,)
total_padding=2088
input_metadata = InputMetadata(prompt_lens=[170, 132, 109, 112], num_prompts=4, max_context_len=None,)
total_padding=2110
input_metadata = InputMetadata(prompt_lens=[116, 94], num_prompts=2, max_context_len=None,)
INFO 12-27 14:21:01 llm_engine.py:666] Avg prompt throughput: 1153.7 tokens/s, Avg generation throughput: 1152.0 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 795 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%
total_padding=2130
input_metadata = InputMetadata(prompt_lens=[98, 88, 88], num_prompts=3, max_context_len=None,)
total_padding=2194
input_metadata = InputMetadata(prompt_lens=[137, 112, 98], num_prompts=3, max_context_len=None,)
total_padding=2194
input_metadata = InputMetadata(prompt_lens=[93], num_prompts=1, max_context_len=None,)
total_padding=2239
input_metadata = InputMetadata(prompt_lens=[128, 111, 100], num_prompts=3, max_context_len=None,)
total_padding=2407
input_metadata = InputMetadata(prompt_lens=[107, 88, 89, 76, 82, 81, 58], num_prompts=7, max_context_len=None,)
total_padding=2411
input_metadata = InputMetadata(prompt_lens=[84, 80], num_prompts=2, max_context_len=None,)
total_padding=2461
input_metadata = InputMetadata(prompt_lens=[91, 67, 90, 84, 73], num_prompts=5, max_context_len=None,)
total_padding=2498
input_metadata = InputMetadata(prompt_lens=[87, 75, 71, 78], num_prompts=4, max_context_len=None,)
total_padding=2537
input_metadata = InputMetadata(prompt_lens=[81, 75, 81, 78, 73, 77, 73, 71], num_prompts=8, max_context_len=None,)
total_padding=2537
input_metadata = InputMetadata(prompt_lens=[81], num_prompts=1, max_context_len=None,)
total_padding=2537
input_metadata = InputMetadata(prompt_lens=[89], num_prompts=1, max_context_len=None,)
total_padding=2614
input_metadata = InputMetadata(prompt_lens=[93, 90, 85, 79, 84, 80, 79, 77], num_prompts=8, max_context_len=None,)
total_padding=2703
input_metadata = InputMetadata(prompt_lens=[55, 65, 75, 64, 63, 68, 63, 64, 74, 70], num_prompts=10, max_context_len=None,)
total_padding=2737
input_metadata = InputMetadata(prompt_lens=[75, 84, 89, 74], num_prompts=4, max_context_len=None,)
total_padding=2778
input_metadata = InputMetadata(prompt_lens=[95, 95, 77, 72], num_prompts=4, max_context_len=None,)
total_padding=2784
input_metadata = InputMetadata(prompt_lens=[139, 133], num_prompts=2, max_context_len=None,)
total_padding=2989
input_metadata = InputMetadata(prompt_lens=[130, 126, 97, 100, 99, 79, 74], num_prompts=7, max_context_len=None,)
INFO 12-27 14:21:06 llm_engine.py:666] Avg prompt throughput: 1258.3 tokens/s, Avg generation throughput: 1149.4 tokens/s, Running: 43 reqs, Swapped: 0 reqs, Pending: 766 reqs, GPU KV cache usage: 98.8%, CPU KV cache usage: 0.0%
total_padding=2994
input_metadata = InputMetadata(prompt_lens=[74, 69], num_prompts=2, max_context_len=None,)
total_padding=3190
input_metadata = InputMetadata(prompt_lens=[112, 91, 84, 81, 75, 68, 77], num_prompts=7, max_context_len=None,)
total_padding=3360
input_metadata = InputMetadata(prompt_lens=[85, 75, 82, 72, 61, 41, 65, 63, 71, 65], num_prompts=10, max_context_len=None,)
total_padding=3375
input_metadata = InputMetadata(prompt_lens=[75, 68, 67], num_prompts=3, max_context_len=None,)
total_padding=3453
input_metadata = InputMetadata(prompt_lens=[58, 59, 67, 64, 57, 61, 70, 64, 66, 60, 68, 68], num_prompts=12, max_context_len=None,)
total_padding=3453
input_metadata = InputMetadata(prompt_lens=[84], num_prompts=1, max_context_len=None,)
total_padding=3467
input_metadata = InputMetadata(prompt_lens=[89, 75], num_prompts=2, max_context_len=None,)
total_padding=3667
input_metadata = InputMetadata(prompt_lens=[107, 103, 95, 79, 78, 77, 78, 71, 75], num_prompts=9, max_context_len=None,)
total_padding=3667
input_metadata = InputMetadata(prompt_lens=[90], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=282, cur_paddings=241, exceeed_seq=56
total_padding=3908
input_metadata = InputMetadata(prompt_lens=[97, 87, 84, 83, 75, 78, 71, 63, 61, 60, 67], num_prompts=11, max_context_len=None,)
total_padding=3918
input_metadata = InputMetadata(prompt_lens=[56, 60, 56, 58], num_prompts=4, max_context_len=None,)
total_padding=4002
input_metadata = InputMetadata(prompt_lens=[60, 58, 61, 61, 54, 56, 48, 55, 51, 51, 32], num_prompts=11, max_context_len=None,)
total_padding=4002
input_metadata = InputMetadata(prompt_lens=[55], num_prompts=1, max_context_len=None,)
total_padding=4028
input_metadata = InputMetadata(prompt_lens=[55, 35, 49], num_prompts=3, max_context_len=None,)
total_padding=4029
input_metadata = InputMetadata(prompt_lens=[81, 80], num_prompts=2, max_context_len=None,)
total_padding=4032
input_metadata = InputMetadata(prompt_lens=[90, 87], num_prompts=2, max_context_len=None,)
total_padding=4032
input_metadata = InputMetadata(prompt_lens=[86], num_prompts=1, max_context_len=None,)
total_padding=4182
input_metadata = InputMetadata(prompt_lens=[90, 83, 71, 64, 66, 63, 43], num_prompts=7, max_context_len=None,)
total_padding=4310
input_metadata = InputMetadata(prompt_lens=[57, 25, 25, 25, 25], num_prompts=5, max_context_len=None,)
total_padding=4310
input_metadata = InputMetadata(prompt_lens=[25, 25, 25], num_prompts=3, max_context_len=None,)
promt select exit padding exceed_paddings=283, cur_paddings=248, exceeed_seq=26
total_padding=4558
input_metadata = InputMetadata(prompt_lens=[61, 25, 25, 25, 26, 26, 26, 26], num_prompts=8, max_context_len=None,)
total_padding=4558
input_metadata = InputMetadata(prompt_lens=[26, 26, 26, 26], num_prompts=4, max_context_len=None,)
total_padding=4558
input_metadata = InputMetadata(prompt_lens=[26, 26, 26, 26, 26], num_prompts=5, max_context_len=None,)
total_padding=4559
input_metadata = InputMetadata(prompt_lens=[26, 27, 27], num_prompts=3, max_context_len=None,)
INFO 12-27 14:21:11 llm_engine.py:666] Avg prompt throughput: 1712.6 tokens/s, Avg generation throughput: 1427.5 tokens/s, Running: 76 reqs, Swapped: 0 reqs, Pending: 697 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 0.0%
total_padding=4572
input_metadata = InputMetadata(prompt_lens=[111, 98], num_prompts=2, max_context_len=None,)
total_padding=4656
input_metadata = InputMetadata(prompt_lens=[142, 58], num_prompts=2, max_context_len=None,)
total_padding=4756
input_metadata = InputMetadata(prompt_lens=[154, 146, 62], num_prompts=3, max_context_len=None,)
total_padding=4842
input_metadata = InputMetadata(prompt_lens=[152, 66], num_prompts=2, max_context_len=None,)
total_padding=5014
input_metadata = InputMetadata(prompt_lens=[167, 156, 70, 141, 129], num_prompts=5, max_context_len=None,)
total_padding=5030
input_metadata = InputMetadata(prompt_lens=[132, 125, 123], num_prompts=3, max_context_len=None,)
total_padding=5030
input_metadata = InputMetadata(prompt_lens=[131], num_prompts=1, max_context_len=None,)
total_padding=5060
input_metadata = InputMetadata(prompt_lens=[144, 142, 133, 127], num_prompts=4, max_context_len=None,)
total_padding=5145
input_metadata = InputMetadata(prompt_lens=[117, 102, 86, 78], num_prompts=4, max_context_len=None,)
total_padding=5193
input_metadata = InputMetadata(prompt_lens=[32, 80], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=261, cur_paddings=198, exceeed_seq=54
total_padding=5391
input_metadata = InputMetadata(prompt_lens=[117, 97, 87, 40, 87, 76], num_prompts=6, max_context_len=None,)
total_padding=5554
input_metadata = InputMetadata(prompt_lens=[54, 65, 27, 27, 27, 27], num_prompts=6, max_context_len=None,)
total_padding=5557
input_metadata = InputMetadata(prompt_lens=[27, 27, 27, 28, 28], num_prompts=5, max_context_len=None,)
promt select exit padding exceed_paddings=278, cur_paddings=210, exceeed_seq=28
total_padding=5767
input_metadata = InputMetadata(prompt_lens=[96, 83, 60, 71, 28, 28], num_prompts=6, max_context_len=None,)
total_padding=5878
input_metadata = InputMetadata(prompt_lens=[28, 28, 28, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 33, 33, 34, 34, 34], num_prompts=35, max_context_len=None,)
total_padding=5878
input_metadata = InputMetadata(prompt_lens=[131], num_prompts=1, max_context_len=None,)
total_padding=5970
input_metadata = InputMetadata(prompt_lens=[132, 188, 185, 175, 168], num_prompts=5, max_context_len=None,)
total_padding=6093
input_metadata = InputMetadata(prompt_lens=[156, 136, 110, 99], num_prompts=4, max_context_len=None,)
INFO 12-27 14:21:16 llm_engine.py:666] Avg prompt throughput: 1666.0 tokens/s, Avg generation throughput: 1837.4 tokens/s, Running: 80 reqs, Swapped: 0 reqs, Pending: 665 reqs, GPU KV cache usage: 98.6%, CPU KV cache usage: 0.0%
total_padding=6229
input_metadata = InputMetadata(prompt_lens=[247, 156, 202], num_prompts=3, max_context_len=None,)
total_padding=6229
input_metadata = InputMetadata(prompt_lens=[204], num_prompts=1, max_context_len=None,)
total_padding=6229
input_metadata = InputMetadata(prompt_lens=[195], num_prompts=1, max_context_len=None,)
total_padding=6274
input_metadata = InputMetadata(prompt_lens=[171, 216], num_prompts=2, max_context_len=None,)
total_padding=6522
input_metadata = InputMetadata(prompt_lens=[205, 184, 176, 162, 141, 114], num_prompts=6, max_context_len=None,)
total_padding=6684
input_metadata = InputMetadata(prompt_lens=[116, 101, 33, 52], num_prompts=4, max_context_len=None,)
total_padding=6684
input_metadata = InputMetadata(prompt_lens=[98], num_prompts=1, max_context_len=None,)
total_padding=6821
input_metadata = InputMetadata(prompt_lens=[85, 62, 72, 34, 35], num_prompts=5, max_context_len=None,)
total_padding=6935
input_metadata = InputMetadata(prompt_lens=[35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 40, 40, 40, 41, 41, 41], num_prompts=33, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[201, 184], num_prompts=2, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[257], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[354], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[84], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[274], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[441], num_prompts=1, max_context_len=None,)
INFO 12-27 14:21:21 llm_engine.py:666] Avg prompt throughput: 1414.3 tokens/s, Avg generation throughput: 2349.8 tokens/s, Running: 68 reqs, Swapped: 0 reqs, Pending: 644 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 0.0%
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[316], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[150], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[196], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[360], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[366], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[318], num_prompts=1, max_context_len=None,)
total_padding=6952
input_metadata = InputMetadata(prompt_lens=[321], num_prompts=1, max_context_len=None,)
total_padding=7026
input_metadata = InputMetadata(prompt_lens=[306, 232], num_prompts=2, max_context_len=None,)
total_padding=7026
input_metadata = InputMetadata(prompt_lens=[237], num_prompts=1, max_context_len=None,)
total_padding=7106
input_metadata = InputMetadata(prompt_lens=[323, 243], num_prompts=2, max_context_len=None,)
total_padding=7106
input_metadata = InputMetadata(prompt_lens=[370], num_prompts=1, max_context_len=None,)
total_padding=7226
input_metadata = InputMetadata(prompt_lens=[376, 256], num_prompts=2, max_context_len=None,)
INFO 12-27 14:21:26 llm_engine.py:666] Avg prompt throughput: 785.2 tokens/s, Avg generation throughput: 1609.0 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 652 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[259, 181], num_prompts=2, max_context_len=None,)
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[281], num_prompts=1, max_context_len=None,)
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[197], num_prompts=1, max_context_len=None,)
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[342], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=271, cur_paddings=0, exceeed_seq=623
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[352], num_prompts=1, max_context_len=None,)
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[623], num_prompts=1, max_context_len=None,)
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[626], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=401, cur_paddings=0, exceeed_seq=231
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[632], num_prompts=1, max_context_len=None,)
total_padding=7304
input_metadata = InputMetadata(prompt_lens=[231], num_prompts=1, max_context_len=None,)
total_padding=7383
input_metadata = InputMetadata(prompt_lens=[237, 158], num_prompts=2, max_context_len=None,)
total_padding=7394
input_metadata = InputMetadata(prompt_lens=[165, 154], num_prompts=2, max_context_len=None,)
total_padding=7552
input_metadata = InputMetadata(prompt_lens=[161, 319], num_prompts=2, max_context_len=None,)
total_padding=7552
input_metadata = InputMetadata(prompt_lens=[324], num_prompts=1, max_context_len=None,)
total_padding=7727
input_metadata = InputMetadata(prompt_lens=[478, 303], num_prompts=2, max_context_len=None,)
total_padding=7727
input_metadata = InputMetadata(prompt_lens=[420], num_prompts=1, max_context_len=None,)
total_padding=7727
input_metadata = InputMetadata(prompt_lens=[460], num_prompts=1, max_context_len=None,)
total_padding=7727
input_metadata = InputMetadata(prompt_lens=[444], num_prompts=1, max_context_len=None,)
total_padding=7900
input_metadata = InputMetadata(prompt_lens=[449, 276], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=282, cur_paddings=0, exceeed_seq=98
total_padding=7900
input_metadata = InputMetadata(prompt_lens=[380], num_prompts=1, max_context_len=None,)
INFO 12-27 14:21:31 llm_engine.py:666] Avg prompt throughput: 1825.4 tokens/s, Avg generation throughput: 1035.6 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 638 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%
total_padding=7900
input_metadata = InputMetadata(prompt_lens=[98], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=294, cur_paddings=11, exceeed_seq=86
total_padding=7911
input_metadata = InputMetadata(prompt_lens=[369, 358], num_prompts=2, max_context_len=None,)
total_padding=7916
input_metadata = InputMetadata(prompt_lens=[86, 81], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=274, cur_paddings=0, exceeed_seq=96
total_padding=7916
input_metadata = InputMetadata(prompt_lens=[370], num_prompts=1, max_context_len=None,)
total_padding=7925
input_metadata = InputMetadata(prompt_lens=[96, 87], num_prompts=2, max_context_len=None,)
total_padding=7930
input_metadata = InputMetadata(prompt_lens=[154, 149], num_prompts=2, max_context_len=None,)
total_padding=8035
input_metadata = InputMetadata(prompt_lens=[220, 260, 239, 216], num_prompts=4, max_context_len=None,)
total_padding=8053
input_metadata = InputMetadata(prompt_lens=[225, 207], num_prompts=2, max_context_len=None,)
total_padding=8135
input_metadata = InputMetadata(prompt_lens=[187, 162, 130], num_prompts=3, max_context_len=None,)
total_padding=8135
input_metadata = InputMetadata(prompt_lens=[113], num_prompts=1, max_context_len=None,)
total_padding=8135
input_metadata = InputMetadata(prompt_lens=[45], num_prompts=1, max_context_len=None,)
total_padding=8260
input_metadata = InputMetadata(prompt_lens=[64, 105, 90, 66, 75], num_prompts=5, max_context_len=None,)
total_padding=8260
input_metadata = InputMetadata(prompt_lens=[96], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=262, cur_paddings=226, exceeed_seq=42
total_padding=8486
input_metadata = InputMetadata(prompt_lens=[71, 78, 41, 41, 41, 42, 42, 42], num_prompts=8, max_context_len=None,)
total_padding=8486
input_metadata = InputMetadata(prompt_lens=[42], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=308, cur_paddings=244, exceeed_seq=44
total_padding=8730
input_metadata = InputMetadata(prompt_lens=[108, 49, 78, 83, 43, 43], num_prompts=6, max_context_len=None,)
total_padding=8730
input_metadata = InputMetadata(prompt_lens=[44], num_prompts=1, max_context_len=None,)
total_padding=8730
input_metadata = InputMetadata(prompt_lens=[86], num_prompts=1, max_context_len=None,)
total_padding=8730
input_metadata = InputMetadata(prompt_lens=[89], num_prompts=1, max_context_len=None,)
total_padding=8762
input_metadata = InputMetadata(prompt_lens=[44, 44, 44, 44, 45, 45, 45, 46, 46, 47, 47, 47, 48], num_prompts=13, max_context_len=None,)
total_padding=8765
input_metadata = InputMetadata(prompt_lens=[49, 49, 49, 50, 50, 50], num_prompts=6, max_context_len=None,)
total_padding=8765
input_metadata = InputMetadata(prompt_lens=[106], num_prompts=1, max_context_len=None,)
total_padding=8883
input_metadata = InputMetadata(prompt_lens=[76, 194], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=311, cur_paddings=77, exceeed_seq=83
total_padding=8960
input_metadata = InputMetadata(prompt_lens=[317, 240], num_prompts=2, max_context_len=None,)
total_padding=8960
input_metadata = InputMetadata(prompt_lens=[83], num_prompts=1, max_context_len=None,)
total_padding=8960
input_metadata = InputMetadata(prompt_lens=[200], num_prompts=1, max_context_len=None,)
INFO 12-27 14:21:36 llm_engine.py:666] Avg prompt throughput: 1677.8 tokens/s, Avg generation throughput: 1191.6 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 595 reqs, GPU KV cache usage: 98.6%, CPU KV cache usage: 0.0%
total_padding=8960
input_metadata = InputMetadata(prompt_lens=[595], num_prompts=1, max_context_len=None,)
total_padding=9210
input_metadata = InputMetadata(prompt_lens=[229, 479], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=437, cur_paddings=0, exceeed_seq=166
total_padding=9210
input_metadata = InputMetadata(prompt_lens=[603], num_prompts=1, max_context_len=None,)
total_padding=9214
input_metadata = InputMetadata(prompt_lens=[166, 162], num_prompts=2, max_context_len=None,)
INFO 12-27 14:21:41 llm_engine.py:666] Avg prompt throughput: 909.7 tokens/s, Avg generation throughput: 1284.2 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 606 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%
promt select exit padding exceed_paddings=444, cur_paddings=0, exceeed_seq=177
total_padding=9214
input_metadata = InputMetadata(prompt_lens=[621], num_prompts=1, max_context_len=None,)
total_padding=9221
input_metadata = InputMetadata(prompt_lens=[177, 170], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=601, cur_paddings=149, exceeed_seq=522
total_padding=9370
input_metadata = InputMetadata(prompt_lens=[296, 147], num_prompts=2, max_context_len=None,)
total_padding=9370
input_metadata = InputMetadata(prompt_lens=[522], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=502, cur_paddings=224, exceeed_seq=89
total_padding=9594
input_metadata = InputMetadata(prompt_lens=[293, 367, 337, 247], num_prompts=4, max_context_len=None,)
total_padding=9594
input_metadata = InputMetadata(prompt_lens=[89], num_prompts=1, max_context_len=None,)
total_padding=9625
input_metadata = InputMetadata(prompt_lens=[203, 172], num_prompts=2, max_context_len=None,)
total_padding=9625
input_metadata = InputMetadata(prompt_lens=[211], num_prompts=1, max_context_len=None,)
total_padding=9721
input_metadata = InputMetadata(prompt_lens=[178, 82], num_prompts=2, max_context_len=None,)
total_padding=9721
input_metadata = InputMetadata(prompt_lens=[227], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=262, cur_paddings=187, exceeed_seq=113
total_padding=9908
input_metadata = InputMetadata(prompt_lens=[188, 91, 98], num_prompts=3, max_context_len=None,)
total_padding=10011
input_metadata = InputMetadata(prompt_lens=[113, 125, 66, 93], num_prompts=4, max_context_len=None,)
total_padding=10071
input_metadata = InputMetadata(prompt_lens=[134, 74], num_prompts=2, max_context_len=None,)
total_padding=10172
input_metadata = InputMetadata(prompt_lens=[99, 96, 50, 50], num_prompts=4, max_context_len=None,)
total_padding=10175
input_metadata = InputMetadata(prompt_lens=[51, 52, 53], num_prompts=3, max_context_len=None,)
total_padding=10178
input_metadata = InputMetadata(prompt_lens=[53, 54, 55, 55, 55], num_prompts=5, max_context_len=None,)
total_padding=10430
input_metadata = InputMetadata(prompt_lens=[145, 152, 91, 111, 105, 56], num_prompts=6, max_context_len=None,)
total_padding=10493
input_metadata = InputMetadata(prompt_lens=[162, 99], num_prompts=2, max_context_len=None,)
total_padding=10623
input_metadata = InputMetadata(prompt_lens=[117, 108, 56, 57], num_prompts=4, max_context_len=None,)
total_padding=10637
input_metadata = InputMetadata(prompt_lens=[57, 57, 57, 57, 57, 58, 58, 58, 58, 59], num_prompts=10, max_context_len=None,)
total_padding=10736
input_metadata = InputMetadata(prompt_lens=[267, 168], num_prompts=2, max_context_len=None,)
total_padding=10840
input_metadata = InputMetadata(prompt_lens=[82, 171, 177, 178], num_prompts=4, max_context_len=None,)
INFO 12-27 14:21:46 llm_engine.py:666] Avg prompt throughput: 2093.8 tokens/s, Avg generation throughput: 1110.3 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 567 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 0.0%
total_padding=10910
input_metadata = InputMetadata(prompt_lens=[183, 113], num_prompts=2, max_context_len=None,)
total_padding=11121
input_metadata = InputMetadata(prompt_lens=[189, 117, 125, 114], num_prompts=4, max_context_len=None,)
total_padding=11125
input_metadata = InputMetadata(prompt_lens=[59, 59, 61], num_prompts=3, max_context_len=None,)
total_padding=11125
input_metadata = InputMetadata(prompt_lens=[62], num_prompts=1, max_context_len=None,)
total_padding=11125
input_metadata = InputMetadata(prompt_lens=[62], num_prompts=1, max_context_len=None,)
total_padding=11125
input_metadata = InputMetadata(prompt_lens=[197], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=358, cur_paddings=219, exceeed_seq=63
total_padding=11344
input_metadata = InputMetadata(prompt_lens=[202, 123, 62], num_prompts=3, max_context_len=None,)
total_padding=11344
input_metadata = InputMetadata(prompt_lens=[63, 63], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=303, cur_paddings=243, exceeed_seq=66
total_padding=11587
input_metadata = InputMetadata(prompt_lens=[67, 126, 64, 64, 66], num_prompts=5, max_context_len=None,)
total_padding=11593
input_metadata = InputMetadata(prompt_lens=[66, 66, 67, 67, 68, 68], num_prompts=6, max_context_len=None,)
promt select exit padding exceed_paddings=280, cur_paddings=225, exceeed_seq=73
total_padding=11818
input_metadata = InputMetadata(prompt_lens=[70, 128, 72, 72, 73], num_prompts=5, max_context_len=None,)
total_padding=11831
input_metadata = InputMetadata(prompt_lens=[73, 74, 76, 77, 77, 78, 78], num_prompts=7, max_context_len=None,)
total_padding=11974
input_metadata = InputMetadata(prompt_lens=[219, 221, 80], num_prompts=3, max_context_len=None,)
total_padding=11975
input_metadata = InputMetadata(prompt_lens=[225, 224], num_prompts=2, max_context_len=None,)
total_padding=11975
input_metadata = InputMetadata(prompt_lens=[82], num_prompts=1, max_context_len=None,)
total_padding=12042
input_metadata = InputMetadata(prompt_lens=[133, 122, 161], num_prompts=3, max_context_len=None,)
total_padding=12042
input_metadata = InputMetadata(prompt_lens=[143], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=265, cur_paddings=0, exceeed_seq=418
total_padding=12042
input_metadata = InputMetadata(prompt_lens=[153], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=272, cur_paddings=0, exceeed_seq=146
total_padding=12042
input_metadata = InputMetadata(prompt_lens=[418], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=270, cur_paddings=54, exceeed_seq=235
total_padding=12096
input_metadata = InputMetadata(prompt_lens=[146, 126, 163], num_prompts=3, max_context_len=None,)
total_padding=12096
input_metadata = InputMetadata(prompt_lens=[235], num_prompts=1, max_context_len=None,)
total_padding=12241
input_metadata = InputMetadata(prompt_lens=[231, 86], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=400, cur_paddings=245, exceeed_seq=79
total_padding=12486
input_metadata = InputMetadata(prompt_lens=[234, 88, 135], num_prompts=3, max_context_len=None,)
total_padding=12494
input_metadata = InputMetadata(prompt_lens=[79, 79, 79, 80, 80, 81, 81], num_prompts=7, max_context_len=None,)
total_padding=12496
input_metadata = InputMetadata(prompt_lens=[82, 82, 83], num_prompts=3, max_context_len=None,)
total_padding=12496
input_metadata = InputMetadata(prompt_lens=[83], num_prompts=1, max_context_len=None,)
total_padding=12496
input_metadata = InputMetadata(prompt_lens=[83], num_prompts=1, max_context_len=None,)
INFO 12-27 14:21:51 llm_engine.py:666] Avg prompt throughput: 1990.7 tokens/s, Avg generation throughput: 1439.9 tokens/s, Running: 57 reqs, Swapped: 0 reqs, Pending: 527 reqs, GPU KV cache usage: 99.4%, CPU KV cache usage: 0.0%
total_padding=12560
input_metadata = InputMetadata(prompt_lens=[185, 249], num_prompts=2, max_context_len=None,)
total_padding=12789
input_metadata = InputMetadata(prompt_lens=[102, 146, 84, 84, 85], num_prompts=5, max_context_len=None,)
total_padding=12919
input_metadata = InputMetadata(prompt_lens=[150, 85, 85], num_prompts=3, max_context_len=None,)
total_padding=12919
input_metadata = InputMetadata(prompt_lens=[98], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=329, cur_paddings=171, exceeed_seq=114
total_padding=13090
input_metadata = InputMetadata(prompt_lens=[272, 101], num_prompts=2, max_context_len=None,)
total_padding=13129
input_metadata = InputMetadata(prompt_lens=[114, 153], num_prompts=2, max_context_len=None,)
total_padding=13129
input_metadata = InputMetadata(prompt_lens=[201], num_prompts=1, max_context_len=None,)
total_padding=13236
input_metadata = InputMetadata(prompt_lens=[318, 425], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=328, cur_paddings=180, exceeed_seq=502
total_padding=13416
input_metadata = InputMetadata(prompt_lens=[428, 248], num_prompts=2, max_context_len=None,)
total_padding=13416
input_metadata = InputMetadata(prompt_lens=[502], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=303, cur_paddings=0, exceeed_seq=207
total_padding=13416
input_metadata = InputMetadata(prompt_lens=[510], num_prompts=1, max_context_len=None,)
total_padding=13439
input_metadata = InputMetadata(prompt_lens=[207, 230], num_prompts=2, max_context_len=None,)
total_padding=13668
input_metadata = InputMetadata(prompt_lens=[232, 287, 113], num_prompts=3, max_context_len=None,)
total_padding=13705
input_metadata = InputMetadata(prompt_lens=[119, 156], num_prompts=2, max_context_len=None,)
total_padding=13814
input_metadata = InputMetadata(prompt_lens=[129, 162, 86], num_prompts=3, max_context_len=None,)
total_padding=13814
input_metadata = InputMetadata(prompt_lens=[135], num_prompts=1, max_context_len=None,)
total_padding=13814
input_metadata = InputMetadata(prompt_lens=[143], num_prompts=1, max_context_len=None,)
INFO 12-27 14:21:56 llm_engine.py:666] Avg prompt throughput: 1632.1 tokens/s, Avg generation throughput: 1431.9 tokens/s, Running: 45 reqs, Swapped: 0 reqs, Pending: 521 reqs, GPU KV cache usage: 99.0%, CPU KV cache usage: 0.0%
total_padding=13858
input_metadata = InputMetadata(prompt_lens=[278, 322], num_prompts=2, max_context_len=None,)
total_padding=13894
input_metadata = InputMetadata(prompt_lens=[289, 325], num_prompts=2, max_context_len=None,)
total_padding=13894
input_metadata = InputMetadata(prompt_lens=[304], num_prompts=1, max_context_len=None,)
total_padding=13894
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
total_padding=13894
input_metadata = InputMetadata(prompt_lens=[332], num_prompts=1, max_context_len=None,)
total_padding=13899
input_metadata = InputMetadata(prompt_lens=[145, 140], num_prompts=2, max_context_len=None,)
total_padding=14057
input_metadata = InputMetadata(prompt_lens=[166, 87, 87], num_prompts=3, max_context_len=None,)
total_padding=14225
input_metadata = InputMetadata(prompt_lens=[172, 88, 88], num_prompts=3, max_context_len=None,)
total_padding=14226
input_metadata = InputMetadata(prompt_lens=[88, 89], num_prompts=2, max_context_len=None,)
total_padding=14332
input_metadata = InputMetadata(prompt_lens=[159, 177, 89], num_prompts=3, max_context_len=None,)
promt select exit padding exceed_paddings=267, cur_paddings=174, exceeed_seq=89
total_padding=14506
input_metadata = InputMetadata(prompt_lens=[116, 167, 182, 89], num_prompts=4, max_context_len=None,)
total_padding=14506
input_metadata = InputMetadata(prompt_lens=[89], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=296, cur_paddings=202, exceeed_seq=92
total_padding=14708
input_metadata = InputMetadata(prompt_lens=[176, 186, 90, 90], num_prompts=4, max_context_len=None,)
total_padding=14708
input_metadata = InputMetadata(prompt_lens=[92], num_prompts=1, max_context_len=None,)
total_padding=14807
input_metadata = InputMetadata(prompt_lens=[191, 92], num_prompts=2, max_context_len=None,)
total_padding=15016
input_metadata = InputMetadata(prompt_lens=[197, 92, 93], num_prompts=3, max_context_len=None,)
promt select exit padding exceed_paddings=314, cur_paddings=210, exceeed_seq=95
total_padding=15226
input_metadata = InputMetadata(prompt_lens=[199, 94, 94], num_prompts=3, max_context_len=None,)
total_padding=15226
input_metadata = InputMetadata(prompt_lens=[95], num_prompts=1, max_context_len=None,)
INFO 12-27 14:22:01 llm_engine.py:666] Avg prompt throughput: 1662.8 tokens/s, Avg generation throughput: 1233.1 tokens/s, Running: 40 reqs, Swapped: 0 reqs, Pending: 506 reqs, GPU KV cache usage: 95.3%, CPU KV cache usage: 0.0%
total_padding=15226
input_metadata = InputMetadata(prompt_lens=[424], num_prompts=1, max_context_len=None,)
total_padding=15311
input_metadata = InputMetadata(prompt_lens=[228, 163, 208], num_prompts=3, max_context_len=None,)
total_padding=15524
input_metadata = InputMetadata(prompt_lens=[202, 95, 96], num_prompts=3, max_context_len=None,)
total_padding=15741
input_metadata = InputMetadata(prompt_lens=[205, 96, 97], num_prompts=3, max_context_len=None,)
total_padding=15743
input_metadata = InputMetadata(prompt_lens=[97, 99], num_prompts=2, max_context_len=None,)
total_padding=15887
input_metadata = InputMetadata(prompt_lens=[226, 209, 99], num_prompts=3, max_context_len=None,)
total_padding=15887
input_metadata = InputMetadata(prompt_lens=[217], num_prompts=1, max_context_len=None,)
total_padding=15887
input_metadata = InputMetadata(prompt_lens=[99], num_prompts=1, max_context_len=None,)
total_padding=15888
input_metadata = InputMetadata(prompt_lens=[99, 100], num_prompts=2, max_context_len=None,)
total_padding=15888
input_metadata = InputMetadata(prompt_lens=[285], num_prompts=1, max_context_len=None,)
total_padding=15968
input_metadata = InputMetadata(prompt_lens=[294, 214], num_prompts=2, max_context_len=None,)
total_padding=15968
input_metadata = InputMetadata(prompt_lens=[223], num_prompts=1, max_context_len=None,)
total_padding=16055
input_metadata = InputMetadata(prompt_lens=[327, 240], num_prompts=2, max_context_len=None,)
total_padding=16062
input_metadata = InputMetadata(prompt_lens=[246, 253], num_prompts=2, max_context_len=None,)
total_padding=16062
input_metadata = InputMetadata(prompt_lens=[261], num_prompts=1, max_context_len=None,)
INFO 12-27 14:22:06 llm_engine.py:666] Avg prompt throughput: 1205.9 tokens/s, Avg generation throughput: 1185.5 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 495 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%
total_padding=16062
input_metadata = InputMetadata(prompt_lens=[585], num_prompts=1, max_context_len=None,)
total_padding=16062
input_metadata = InputMetadata(prompt_lens=[373], num_prompts=1, max_context_len=None,)
total_padding=16076
input_metadata = InputMetadata(prompt_lens=[273, 259], num_prompts=2, max_context_len=None,)
total_padding=16281
input_metadata = InputMetadata(prompt_lens=[266, 226, 101], num_prompts=3, max_context_len=None,)
total_padding=16505
input_metadata = InputMetadata(prompt_lens=[279, 233, 101], num_prompts=3, max_context_len=None,)
promt select exit padding exceed_paddings=347, cur_paddings=121, exceeed_seq=101
total_padding=16626
input_metadata = InputMetadata(prompt_lens=[327, 292, 241], num_prompts=3, max_context_len=None,)
total_padding=16626
input_metadata = InputMetadata(prompt_lens=[101, 101], num_prompts=2, max_context_len=None,)
total_padding=16626
input_metadata = InputMetadata(prompt_lens=[103], num_prompts=1, max_context_len=None,)
total_padding=16626
input_metadata = InputMetadata(prompt_lens=[253], num_prompts=1, max_context_len=None,)
total_padding=16626
input_metadata = InputMetadata(prompt_lens=[104], num_prompts=1, max_context_len=None,)
total_padding=16626
input_metadata = InputMetadata(prompt_lens=[105], num_prompts=1, max_context_len=None,)
total_padding=16630
input_metadata = InputMetadata(prompt_lens=[106, 107, 107, 108, 108], num_prompts=5, max_context_len=None,)
promt select exit padding exceed_paddings=377, cur_paddings=155, exceeed_seq=267
total_padding=16785
input_metadata = InputMetadata(prompt_lens=[489, 334], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=316, cur_paddings=158, exceeed_seq=109
total_padding=16943
input_metadata = InputMetadata(prompt_lens=[267, 109], num_prompts=2, max_context_len=None,)
total_padding=16956
input_metadata = InputMetadata(prompt_lens=[109, 112, 112, 114, 115], num_prompts=5, max_context_len=None,)
INFO 12-27 14:22:11 llm_engine.py:666] Avg prompt throughput: 1476.9 tokens/s, Avg generation throughput: 1032.2 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 475 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 0.0%
total_padding=16956
input_metadata = InputMetadata(prompt_lens=[511], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=299, cur_paddings=73, exceeed_seq=118
total_padding=17029
input_metadata = InputMetadata(prompt_lens=[344, 271], num_prompts=2, max_context_len=None,)
total_padding=17029
input_metadata = InputMetadata(prompt_lens=[118], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=312, cur_paddings=157, exceeed_seq=121
total_padding=17186
input_metadata = InputMetadata(prompt_lens=[276, 119], num_prompts=2, max_context_len=None,)
total_padding=17199
input_metadata = InputMetadata(prompt_lens=[121, 121, 121, 122, 123, 123, 124], num_prompts=7, max_context_len=None,)
promt select exit padding exceed_paddings=320, cur_paddings=160, exceeed_seq=126
total_padding=17359
input_metadata = InputMetadata(prompt_lens=[286, 126], num_prompts=2, max_context_len=None,)
total_padding=17364
input_metadata = InputMetadata(prompt_lens=[126, 131, 131], num_prompts=3, max_context_len=None,)
promt select exit padding exceed_paddings=298, cur_paddings=140, exceeed_seq=131
total_padding=17504
input_metadata = InputMetadata(prompt_lens=[149, 289], num_prompts=2, max_context_len=None,)
total_padding=17504
input_metadata = InputMetadata(prompt_lens=[131, 131], num_prompts=2, max_context_len=None,)
total_padding=17504
input_metadata = InputMetadata(prompt_lens=[131], num_prompts=1, max_context_len=None,)
total_padding=17505
input_metadata = InputMetadata(prompt_lens=[132, 133], num_prompts=2, max_context_len=None,)
total_padding=17505
input_metadata = InputMetadata(prompt_lens=[301], num_prompts=1, max_context_len=None,)
total_padding=17508
input_metadata = InputMetadata(prompt_lens=[134, 135, 136], num_prompts=3, max_context_len=None,)
total_padding=17512
input_metadata = InputMetadata(prompt_lens=[136, 137, 137, 138], num_prompts=4, max_context_len=None,)
promt select exit padding exceed_paddings=304, cur_paddings=129, exceeed_seq=138
total_padding=17641
input_metadata = InputMetadata(prompt_lens=[184, 313], num_prompts=2, max_context_len=None,)
total_padding=17643
input_metadata = InputMetadata(prompt_lens=[138, 140], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=351, cur_paddings=176, exceeed_seq=143
total_padding=17819
input_metadata = InputMetadata(prompt_lens=[318, 142], num_prompts=2, max_context_len=None,)
total_padding=17820
input_metadata = InputMetadata(prompt_lens=[143, 144], num_prompts=2, max_context_len=None,)
total_padding=18000
input_metadata = InputMetadata(prompt_lens=[326, 146], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=365, cur_paddings=183, exceeed_seq=148
total_padding=18183
input_metadata = InputMetadata(prompt_lens=[330, 147], num_prompts=2, max_context_len=None,)
total_padding=18183
input_metadata = InputMetadata(prompt_lens=[148], num_prompts=1, max_context_len=None,)
total_padding=18183
input_metadata = InputMetadata(prompt_lens=[230], num_prompts=1, max_context_len=None,)
INFO 12-27 14:22:16 llm_engine.py:666] Avg prompt throughput: 1984.1 tokens/s, Avg generation throughput: 1174.4 tokens/s, Running: 43 reqs, Swapped: 0 reqs, Pending: 441 reqs, GPU KV cache usage: 98.4%, CPU KV cache usage: 0.0%
total_padding=18429
input_metadata = InputMetadata(prompt_lens=[484, 238], num_prompts=2, max_context_len=None,)
total_padding=18429
input_metadata = InputMetadata(prompt_lens=[292], num_prompts=1, max_context_len=None,)
total_padding=18429
input_metadata = InputMetadata(prompt_lens=[304], num_prompts=1, max_context_len=None,)
total_padding=18681
input_metadata = InputMetadata(prompt_lens=[494, 242], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=377, cur_paddings=189, exceeed_seq=149
total_padding=18870
input_metadata = InputMetadata(prompt_lens=[337, 148], num_prompts=2, max_context_len=None,)
total_padding=18871
input_metadata = InputMetadata(prompt_lens=[149, 150], num_prompts=2, max_context_len=None,)
total_padding=18871
input_metadata = InputMetadata(prompt_lens=[151], num_prompts=1, max_context_len=None,)
INFO 12-27 14:22:21 llm_engine.py:666] Avg prompt throughput: 783.3 tokens/s, Avg generation throughput: 1186.4 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 440 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%
total_padding=18871
input_metadata = InputMetadata(prompt_lens=[376], num_prompts=1, max_context_len=None,)
total_padding=18871
input_metadata = InputMetadata(prompt_lens=[387], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=391, cur_paddings=133, exceeed_seq=271
total_padding=19004
input_metadata = InputMetadata(prompt_lens=[513, 412, 529], num_prompts=3, max_context_len=None,)
total_padding=19004
input_metadata = InputMetadata(prompt_lens=[271], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=388, cur_paddings=196, exceeed_seq=156
total_padding=19200
input_metadata = InputMetadata(prompt_lens=[348, 152], num_prompts=2, max_context_len=None,)
total_padding=19203
input_metadata = InputMetadata(prompt_lens=[156, 159], num_prompts=2, max_context_len=None,)
total_padding=19208
input_metadata = InputMetadata(prompt_lens=[159, 164], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=386, cur_paddings=193, exceeed_seq=164
total_padding=19401
input_metadata = InputMetadata(prompt_lens=[357, 164], num_prompts=2, max_context_len=None,)
total_padding=19401
input_metadata = InputMetadata(prompt_lens=[164], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=409, cur_paddings=198, exceeed_seq=164
total_padding=19599
input_metadata = InputMetadata(prompt_lens=[375, 177], num_prompts=2, max_context_len=None,)
total_padding=19601
input_metadata = InputMetadata(prompt_lens=[164, 166], num_prompts=2, max_context_len=None,)
total_padding=19601
input_metadata = InputMetadata(prompt_lens=[167, 167, 167], num_prompts=3, max_context_len=None,)
promt select exit padding exceed_paddings=613, cur_paddings=205, exceeed_seq=191
total_padding=19806
input_metadata = InputMetadata(prompt_lens=[599, 394], num_prompts=2, max_context_len=None,)
total_padding=19806
input_metadata = InputMetadata(prompt_lens=[191], num_prompts=1, max_context_len=None,)
total_padding=20014
input_metadata = InputMetadata(prompt_lens=[404, 196], num_prompts=2, max_context_len=None,)
INFO 12-27 14:22:26 llm_engine.py:666] Avg prompt throughput: 1574.0 tokens/s, Avg generation throughput: 971.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 425 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 0.0%
total_padding=20014
input_metadata = InputMetadata(prompt_lens=[417], num_prompts=1, max_context_len=None,)
total_padding=20240
input_metadata = InputMetadata(prompt_lens=[428, 202], num_prompts=2, max_context_len=None,)
total_padding=20475
input_metadata = InputMetadata(prompt_lens=[445, 210], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=534, cur_paddings=242, exceeed_seq=167
total_padding=20717
input_metadata = InputMetadata(prompt_lens=[459, 217], num_prompts=2, max_context_len=None,)
total_padding=20717
input_metadata = InputMetadata(prompt_lens=[167], num_prompts=1, max_context_len=None,)
total_padding=20717
input_metadata = InputMetadata(prompt_lens=[725], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=563, cur_paddings=251, exceeed_seq=167
total_padding=20968
input_metadata = InputMetadata(prompt_lens=[479, 228], num_prompts=2, max_context_len=None,)
total_padding=20968
input_metadata = InputMetadata(prompt_lens=[167, 167, 167], num_prompts=3, max_context_len=None,)
promt select exit padding exceed_paddings=261, cur_paddings=198, exceeed_seq=171
total_padding=21166
input_metadata = InputMetadata(prompt_lens=[234, 168, 168, 168], num_prompts=4, max_context_len=None,)
total_padding=21166
input_metadata = InputMetadata(prompt_lens=[171], num_prompts=1, max_context_len=None,)
total_padding=21238
input_metadata = InputMetadata(prompt_lens=[243, 171], num_prompts=2, max_context_len=None,)
total_padding=21238
input_metadata = InputMetadata(prompt_lens=[175], num_prompts=1, max_context_len=None,)
total_padding=21238
input_metadata = InputMetadata(prompt_lens=[175], num_prompts=1, max_context_len=None,)
INFO 12-27 14:22:32 llm_engine.py:666] Avg prompt throughput: 1599.2 tokens/s, Avg generation throughput: 940.7 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 414 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%
total_padding=21238
input_metadata = InputMetadata(prompt_lens=[260], num_prompts=1, max_context_len=None,)
total_padding=21241
input_metadata = InputMetadata(prompt_lens=[175, 178, 178], num_prompts=3, max_context_len=None,)
total_padding=21241
input_metadata = InputMetadata(prompt_lens=[179], num_prompts=1, max_context_len=None,)
total_padding=21243
input_metadata = InputMetadata(prompt_lens=[180, 180, 181], num_prompts=3, max_context_len=None,)
total_padding=21243
input_metadata = InputMetadata(prompt_lens=[801], num_prompts=1, max_context_len=None,)
total_padding=21243
input_metadata = InputMetadata(prompt_lens=[291], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=348, cur_paddings=233, exceeed_seq=184
total_padding=21476
input_metadata = InputMetadata(prompt_lens=[299, 182, 183], num_prompts=3, max_context_len=None,)
total_padding=21476
input_metadata = InputMetadata(prompt_lens=[184], num_prompts=1, max_context_len=None,)
total_padding=21476
input_metadata = InputMetadata(prompt_lens=[308], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=259, cur_paddings=130, exceeed_seq=185
total_padding=21606
input_metadata = InputMetadata(prompt_lens=[314, 184], num_prompts=2, max_context_len=None,)
total_padding=21608
input_metadata = InputMetadata(prompt_lens=[185, 187], num_prompts=2, max_context_len=None,)
total_padding=21612
input_metadata = InputMetadata(prompt_lens=[189, 189, 191], num_prompts=3, max_context_len=None,)
total_padding=21614
input_metadata = InputMetadata(prompt_lens=[192, 194, 194], num_prompts=3, max_context_len=None,)
total_padding=21614
input_metadata = InputMetadata(prompt_lens=[206], num_prompts=1, max_context_len=None,)
total_padding=21617
input_metadata = InputMetadata(prompt_lens=[194, 197], num_prompts=2, max_context_len=None,)
total_padding=21617
input_metadata = InputMetadata(prompt_lens=[805], num_prompts=1, max_context_len=None,)
total_padding=21617
input_metadata = InputMetadata(prompt_lens=[215], num_prompts=1, max_context_len=None,)
total_padding=21629
input_metadata = InputMetadata(prompt_lens=[197, 201, 202, 204], num_prompts=4, max_context_len=None,)
INFO 12-27 14:22:37 llm_engine.py:666] Avg prompt throughput: 1644.8 tokens/s, Avg generation throughput: 959.1 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 390 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 0.0%
total_padding=21678
input_metadata = InputMetadata(prompt_lens=[221, 206, 209, 210, 210], num_prompts=5, max_context_len=None,)
promt select exit padding exceed_paddings=615, cur_paddings=0, exceeed_seq=227
total_padding=21678
input_metadata = InputMetadata(prompt_lens=[842], num_prompts=1, max_context_len=None,)
total_padding=21678
input_metadata = InputMetadata(prompt_lens=[227], num_prompts=1, max_context_len=None,)
total_padding=21700
input_metadata = InputMetadata(prompt_lens=[210, 211, 211, 218], num_prompts=4, max_context_len=None,)
promt select exit padding exceed_paddings=622, cur_paddings=0, exceeed_seq=238
total_padding=21700
input_metadata = InputMetadata(prompt_lens=[860], num_prompts=1, max_context_len=None,)
total_padding=21700
input_metadata = InputMetadata(prompt_lens=[238], num_prompts=1, max_context_len=None,)
total_padding=21700
input_metadata = InputMetadata(prompt_lens=[875], num_prompts=1, max_context_len=None,)
total_padding=21700
input_metadata = InputMetadata(prompt_lens=[884], num_prompts=1, max_context_len=None,)
total_padding=21700
input_metadata = InputMetadata(prompt_lens=[894], num_prompts=1, max_context_len=None,)
total_padding=21700
input_metadata = InputMetadata(prompt_lens=[245], num_prompts=1, max_context_len=None,)
total_padding=21734
input_metadata = InputMetadata(prompt_lens=[253, 219], num_prompts=2, max_context_len=None,)
total_padding=21819
input_metadata = InputMetadata(prompt_lens=[264, 220, 223], num_prompts=3, max_context_len=None,)
total_padding=21825
input_metadata = InputMetadata(prompt_lens=[224, 226, 228, 228], num_prompts=4, max_context_len=None,)
total_padding=21951
input_metadata = InputMetadata(prompt_lens=[274, 232, 232, 232], num_prompts=4, max_context_len=None,)
INFO 12-27 14:22:42 llm_engine.py:666] Avg prompt throughput: 1888.0 tokens/s, Avg generation throughput: 961.8 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 371 reqs, GPU KV cache usage: 98.6%, CPU KV cache usage: 0.0%
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[233, 234, 234], num_prompts=3, max_context_len=None,)
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[235], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=671, cur_paddings=0, exceeed_seq=288
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[959], num_prompts=1, max_context_len=None,)
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[288], num_prompts=1, max_context_len=None,)
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[991], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=698, cur_paddings=0, exceeed_seq=301
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[999], num_prompts=1, max_context_len=None,)
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[301], num_prompts=1, max_context_len=None,)
total_padding=21952
input_metadata = InputMetadata(prompt_lens=[309], num_prompts=1, max_context_len=None,)
total_padding=22191
input_metadata = InputMetadata(prompt_lens=[317, 236, 236, 240], num_prompts=4, max_context_len=None,)
INFO 12-27 14:22:47 llm_engine.py:666] Avg prompt throughput: 1009.4 tokens/s, Avg generation throughput: 1007.4 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 365 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 0.0%
total_padding=22191
input_metadata = InputMetadata(prompt_lens=[1051], num_prompts=1, max_context_len=None,)
total_padding=22191
input_metadata = InputMetadata(prompt_lens=[1060], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=750, cur_paddings=0, exceeed_seq=322
total_padding=22191
input_metadata = InputMetadata(prompt_lens=[1072], num_prompts=1, max_context_len=None,)
total_padding=22191
input_metadata = InputMetadata(prompt_lens=[322], num_prompts=1, max_context_len=None,)
total_padding=22191
input_metadata = InputMetadata(prompt_lens=[243], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=294, cur_paddings=197, exceeed_seq=245
total_padding=22388
input_metadata = InputMetadata(prompt_lens=[342, 243, 244], num_prompts=3, max_context_len=None,)
total_padding=22388
input_metadata = InputMetadata(prompt_lens=[245], num_prompts=1, max_context_len=None,)
total_padding=22388
input_metadata = InputMetadata(prompt_lens=[245, 245], num_prompts=2, max_context_len=None,)
total_padding=22388
input_metadata = InputMetadata(prompt_lens=[248], num_prompts=1, max_context_len=None,)
total_padding=22388
input_metadata = InputMetadata(prompt_lens=[248], num_prompts=1, max_context_len=None,)
total_padding=22396
input_metadata = InputMetadata(prompt_lens=[249, 249, 253], num_prompts=3, max_context_len=None,)
total_padding=22397
input_metadata = InputMetadata(prompt_lens=[255, 256], num_prompts=2, max_context_len=None,)
total_padding=22397
input_metadata = InputMetadata(prompt_lens=[259], num_prompts=1, max_context_len=None,)
total_padding=22397
input_metadata = InputMetadata(prompt_lens=[262], num_prompts=1, max_context_len=None,)
total_padding=22397
input_metadata = InputMetadata(prompt_lens=[262], num_prompts=1, max_context_len=None,)
total_padding=22408
input_metadata = InputMetadata(prompt_lens=[263, 264, 269], num_prompts=3, max_context_len=None,)
total_padding=22411
input_metadata = InputMetadata(prompt_lens=[271, 274], num_prompts=2, max_context_len=None,)
total_padding=22411
input_metadata = InputMetadata(prompt_lens=[276], num_prompts=1, max_context_len=None,)
total_padding=22411
input_metadata = InputMetadata(prompt_lens=[277], num_prompts=1, max_context_len=None,)
INFO 12-27 14:22:52 llm_engine.py:666] Avg prompt throughput: 2269.1 tokens/s, Avg generation throughput: 772.7 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 341 reqs, GPU KV cache usage: 97.8%, CPU KV cache usage: 0.0%
total_padding=22411
input_metadata = InputMetadata(prompt_lens=[278], num_prompts=1, max_context_len=None,)
total_padding=22411
input_metadata = InputMetadata(prompt_lens=[282, 282], num_prompts=2, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[283, 285], num_prompts=2, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[286], num_prompts=1, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[288], num_prompts=1, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[429], num_prompts=1, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[291], num_prompts=1, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[451], num_prompts=1, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[294], num_prompts=1, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[494], num_prompts=1, max_context_len=None,)
total_padding=22413
input_metadata = InputMetadata(prompt_lens=[301], num_prompts=1, max_context_len=None,)
total_padding=22415
input_metadata = InputMetadata(prompt_lens=[301, 301, 302], num_prompts=3, max_context_len=None,)
total_padding=22415
input_metadata = InputMetadata(prompt_lens=[518], num_prompts=1, max_context_len=None,)
total_padding=22415
input_metadata = InputMetadata(prompt_lens=[303], num_prompts=1, max_context_len=None,)
total_padding=22415
input_metadata = InputMetadata(prompt_lens=[304], num_prompts=1, max_context_len=None,)
INFO 12-27 14:22:57 llm_engine.py:666] Avg prompt throughput: 1254.7 tokens/s, Avg generation throughput: 799.4 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 327 reqs, GPU KV cache usage: 98.5%, CPU KV cache usage: 0.0%
total_padding=22415
input_metadata = InputMetadata(prompt_lens=[535], num_prompts=1, max_context_len=None,)
total_padding=22417
input_metadata = InputMetadata(prompt_lens=[304, 306], num_prompts=2, max_context_len=None,)
promt select exit padding exceed_paddings=570, cur_paddings=0, exceeed_seq=307
total_padding=22417
input_metadata = InputMetadata(prompt_lens=[877], num_prompts=1, max_context_len=None,)
total_padding=22417
input_metadata = InputMetadata(prompt_lens=[307], num_prompts=1, max_context_len=None,)
total_padding=22419
input_metadata = InputMetadata(prompt_lens=[308, 308, 309], num_prompts=3, max_context_len=None,)
total_padding=22419
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
total_padding=22419
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
total_padding=22419
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
total_padding=22419
input_metadata = InputMetadata(prompt_lens=[342], num_prompts=1, max_context_len=None,)
total_padding=22419
input_metadata = InputMetadata(prompt_lens=[356], num_prompts=1, max_context_len=None,)
total_padding=22427
input_metadata = InputMetadata(prompt_lens=[310, 310, 314], num_prompts=3, max_context_len=None,)
total_padding=22429
input_metadata = InputMetadata(prompt_lens=[314, 316], num_prompts=2, max_context_len=None,)
total_padding=22429
input_metadata = InputMetadata(prompt_lens=[320], num_prompts=1, max_context_len=None,)
total_padding=22429
input_metadata = InputMetadata(prompt_lens=[320], num_prompts=1, max_context_len=None,)
total_padding=22430
input_metadata = InputMetadata(prompt_lens=[322, 323, 323], num_prompts=3, max_context_len=None,)
total_padding=22430
input_metadata = InputMetadata(prompt_lens=[325], num_prompts=1, max_context_len=None,)
total_padding=22430
input_metadata = InputMetadata(prompt_lens=[325], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[328, 329, 329, 329], num_prompts=4, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[330], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:02 llm_engine.py:666] Avg prompt throughput: 2103.9 tokens/s, Avg generation throughput: 680.9 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 301 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[390], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[331], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[332], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[333], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[337], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[428], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[338], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[338], num_prompts=1, max_context_len=None,)
total_padding=22431
input_metadata = InputMetadata(prompt_lens=[442], num_prompts=1, max_context_len=None,)
total_padding=22543
input_metadata = InputMetadata(prompt_lens=[453, 341], num_prompts=2, max_context_len=None,)
total_padding=22547
input_metadata = InputMetadata(prompt_lens=[342, 342, 344], num_prompts=3, max_context_len=None,)
total_padding=22547
input_metadata = InputMetadata(prompt_lens=[344], num_prompts=1, max_context_len=None,)
total_padding=22547
input_metadata = InputMetadata(prompt_lens=[348], num_prompts=1, max_context_len=None,)
total_padding=22547
input_metadata = InputMetadata(prompt_lens=[359], num_prompts=1, max_context_len=None,)
total_padding=22547
input_metadata = InputMetadata(prompt_lens=[348], num_prompts=1, max_context_len=None,)
total_padding=22548
input_metadata = InputMetadata(prompt_lens=[349, 350], num_prompts=2, max_context_len=None,)
total_padding=22548
input_metadata = InputMetadata(prompt_lens=[352], num_prompts=1, max_context_len=None,)
total_padding=22548
input_metadata = InputMetadata(prompt_lens=[353], num_prompts=1, max_context_len=None,)
total_padding=22548
input_metadata = InputMetadata(prompt_lens=[355], num_prompts=1, max_context_len=None,)
total_padding=22548
input_metadata = InputMetadata(prompt_lens=[356], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:07 llm_engine.py:666] Avg prompt throughput: 1721.9 tokens/s, Avg generation throughput: 701.9 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 281 reqs, GPU KV cache usage: 87.3%, CPU KV cache usage: 0.0%
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[358, 358, 360, 364], num_prompts=4, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[365], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[366], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[366], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[367], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[369], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[370], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[373], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[390], num_prompts=1, max_context_len=None,)
total_padding=22564
input_metadata = InputMetadata(prompt_lens=[373], num_prompts=1, max_context_len=None,)
total_padding=22567
input_metadata = InputMetadata(prompt_lens=[375, 376, 377], num_prompts=3, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[378, 380], num_prompts=2, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[381], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[382, 382], num_prompts=2, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[383], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[384], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[385], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[387], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[387], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[387], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[390], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[390], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[391], num_prompts=1, max_context_len=None,)
total_padding=22569
input_metadata = InputMetadata(prompt_lens=[432], num_prompts=1, max_context_len=None,)
total_padding=22574
input_metadata = InputMetadata(prompt_lens=[392, 397, 397], num_prompts=3, max_context_len=None,)
INFO 12-27 14:23:12 llm_engine.py:666] Avg prompt throughput: 2407.1 tokens/s, Avg generation throughput: 647.7 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 249 reqs, GPU KV cache usage: 97.9%, CPU KV cache usage: 0.0%
total_padding=22574
input_metadata = InputMetadata(prompt_lens=[397], num_prompts=1, max_context_len=None,)
total_padding=22579
input_metadata = InputMetadata(prompt_lens=[399, 404], num_prompts=2, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[406, 408, 408], num_prompts=3, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[450], num_prompts=1, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[410], num_prompts=1, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[413], num_prompts=1, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[466], num_prompts=1, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[413, 413], num_prompts=2, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[415], num_prompts=1, max_context_len=None,)
total_padding=22581
input_metadata = InputMetadata(prompt_lens=[415], num_prompts=1, max_context_len=None,)
total_padding=22647
input_metadata = InputMetadata(prompt_lens=[481, 415], num_prompts=2, max_context_len=None,)
total_padding=22647
input_metadata = InputMetadata(prompt_lens=[418], num_prompts=1, max_context_len=None,)
total_padding=22647
input_metadata = InputMetadata(prompt_lens=[419], num_prompts=1, max_context_len=None,)
total_padding=22647
input_metadata = InputMetadata(prompt_lens=[513], num_prompts=1, max_context_len=None,)
total_padding=22647
input_metadata = InputMetadata(prompt_lens=[421, 421], num_prompts=2, max_context_len=None,)
total_padding=22647
input_metadata = InputMetadata(prompt_lens=[422], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:17 llm_engine.py:666] Avg prompt throughput: 1815.3 tokens/s, Avg generation throughput: 730.1 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 231 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 0.0%
total_padding=22647
input_metadata = InputMetadata(prompt_lens=[423], num_prompts=1, max_context_len=None,)
total_padding=22648
input_metadata = InputMetadata(prompt_lens=[424, 425], num_prompts=2, max_context_len=None,)
total_padding=22648
input_metadata = InputMetadata(prompt_lens=[427], num_prompts=1, max_context_len=None,)
total_padding=22648
input_metadata = InputMetadata(prompt_lens=[432], num_prompts=1, max_context_len=None,)
total_padding=22648
input_metadata = InputMetadata(prompt_lens=[435], num_prompts=1, max_context_len=None,)
total_padding=22648
input_metadata = InputMetadata(prompt_lens=[438], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[438, 439, 439], num_prompts=3, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[441], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[444], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[447], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[451], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[451], num_prompts=1, max_context_len=None,)
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[452], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:22 llm_engine.py:666] Avg prompt throughput: 2131.1 tokens/s, Avg generation throughput: 620.6 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 207 reqs, GPU KV cache usage: 99.4%, CPU KV cache usage: 0.0%
promt select exit padding exceed_paddings=274, cur_paddings=0, exceeed_seq=452
total_padding=22649
input_metadata = InputMetadata(prompt_lens=[726], num_prompts=1, max_context_len=None,)
total_padding=22654
input_metadata = InputMetadata(prompt_lens=[452, 457], num_prompts=2, max_context_len=None,)
total_padding=22654
input_metadata = InputMetadata(prompt_lens=[457], num_prompts=1, max_context_len=None,)
total_padding=22654
input_metadata = InputMetadata(prompt_lens=[458, 458], num_prompts=2, max_context_len=None,)
total_padding=22655
input_metadata = InputMetadata(prompt_lens=[458, 459], num_prompts=2, max_context_len=None,)
total_padding=22657
input_metadata = InputMetadata(prompt_lens=[460, 460, 461], num_prompts=3, max_context_len=None,)
total_padding=22657
input_metadata = InputMetadata(prompt_lens=[756], num_prompts=1, max_context_len=None,)
total_padding=22657
input_metadata = InputMetadata(prompt_lens=[461], num_prompts=1, max_context_len=None,)
total_padding=22657
input_metadata = InputMetadata(prompt_lens=[461], num_prompts=1, max_context_len=None,)
total_padding=22660
input_metadata = InputMetadata(prompt_lens=[463, 464, 465], num_prompts=3, max_context_len=None,)
total_padding=22660
input_metadata = InputMetadata(prompt_lens=[468], num_prompts=1, max_context_len=None,)
total_padding=22660
input_metadata = InputMetadata(prompt_lens=[468], num_prompts=1, max_context_len=None,)
total_padding=22662
input_metadata = InputMetadata(prompt_lens=[468, 470], num_prompts=2, max_context_len=None,)
total_padding=22662
input_metadata = InputMetadata(prompt_lens=[471], num_prompts=1, max_context_len=None,)
total_padding=22662
input_metadata = InputMetadata(prompt_lens=[475], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[475, 476, 481, 482], num_prompts=4, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[487], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[489], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[490], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[493], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[496], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[498], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:27 llm_engine.py:666] Avg prompt throughput: 3242.0 tokens/s, Avg generation throughput: 563.0 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 176 reqs, GPU KV cache usage: 93.6%, CPU KV cache usage: 0.0%
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[498], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[500], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[501], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[504], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[504], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[505], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[506], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[509], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[510], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[512], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[517], num_prompts=1, max_context_len=None,)
total_padding=22676
input_metadata = InputMetadata(prompt_lens=[518], num_prompts=1, max_context_len=None,)
total_padding=22753
input_metadata = InputMetadata(prompt_lens=[596, 519], num_prompts=2, max_context_len=None,)
total_padding=22753
input_metadata = InputMetadata(prompt_lens=[520], num_prompts=1, max_context_len=None,)
total_padding=22753
input_metadata = InputMetadata(prompt_lens=[521], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[618, 523], num_prompts=2, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[523], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[525], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[529], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[643], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[534], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[536], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:32 llm_engine.py:666] Avg prompt throughput: 2797.2 tokens/s, Avg generation throughput: 601.1 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 152 reqs, GPU KV cache usage: 98.2%, CPU KV cache usage: 0.0%
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[536], num_prompts=1, max_context_len=None,)
total_padding=22848
input_metadata = InputMetadata(prompt_lens=[538], num_prompts=1, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[539, 540], num_prompts=2, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[541], num_prompts=1, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[543], num_prompts=1, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[549], num_prompts=1, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[550], num_prompts=1, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[551], num_prompts=1, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[554], num_prompts=1, max_context_len=None,)
total_padding=22849
input_metadata = InputMetadata(prompt_lens=[554], num_prompts=1, max_context_len=None,)
total_padding=22850
input_metadata = InputMetadata(prompt_lens=[558, 559], num_prompts=2, max_context_len=None,)
total_padding=22850
input_metadata = InputMetadata(prompt_lens=[560], num_prompts=1, max_context_len=None,)
total_padding=22853
input_metadata = InputMetadata(prompt_lens=[560, 563], num_prompts=2, max_context_len=None,)
total_padding=22859
input_metadata = InputMetadata(prompt_lens=[564, 570], num_prompts=2, max_context_len=None,)
total_padding=22859
input_metadata = InputMetadata(prompt_lens=[578], num_prompts=1, max_context_len=None,)
total_padding=22859
input_metadata = InputMetadata(prompt_lens=[580], num_prompts=1, max_context_len=None,)
total_padding=22859
input_metadata = InputMetadata(prompt_lens=[580], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[584, 585], num_prompts=2, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[587], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:37 llm_engine.py:666] Avg prompt throughput: 2605.0 tokens/s, Avg generation throughput: 560.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 128 reqs, GPU KV cache usage: 95.7%, CPU KV cache usage: 0.0%
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[593], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[596], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[597], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[598], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[599], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[840], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[872], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[601], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[604], num_prompts=1, max_context_len=None,)
promt select exit padding exceed_paddings=289, cur_paddings=0, exceeed_seq=609
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[898], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[609], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:42 llm_engine.py:666] Avg prompt throughput: 1410.9 tokens/s, Avg generation throughput: 612.7 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 120 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 0.0%
promt select exit padding exceed_paddings=305, cur_paddings=0, exceeed_seq=609
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[914], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[609], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[611], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[612], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[613], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[616], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[616], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[622], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[624], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[627], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[627], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[629], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[633], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[637], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[652], num_prompts=1, max_context_len=None,)
total_padding=22860
input_metadata = InputMetadata(prompt_lens=[656], num_prompts=1, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[658, 661], num_prompts=2, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[662], num_prompts=1, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[664], num_prompts=1, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[665], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:47 llm_engine.py:666] Avg prompt throughput: 2766.8 tokens/s, Avg generation throughput: 469.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 100 reqs, GPU KV cache usage: 98.6%, CPU KV cache usage: 0.0%
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[668], num_prompts=1, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[668], num_prompts=1, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[672], num_prompts=1, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[672], num_prompts=1, max_context_len=None,)
total_padding=22863
input_metadata = InputMetadata(prompt_lens=[673, 673], num_prompts=2, max_context_len=None,)
total_padding=22866
input_metadata = InputMetadata(prompt_lens=[675, 678], num_prompts=2, max_context_len=None,)
total_padding=22866
input_metadata = InputMetadata(prompt_lens=[1090], num_prompts=1, max_context_len=None,)
total_padding=22866
input_metadata = InputMetadata(prompt_lens=[682], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[684, 686], num_prompts=2, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[686], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[686], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[690], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[692], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[696], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[697], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[701], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[703], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[706], num_prompts=1, max_context_len=None,)
total_padding=22868
input_metadata = InputMetadata(prompt_lens=[719], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:52 llm_engine.py:666] Avg prompt throughput: 3008.4 tokens/s, Avg generation throughput: 428.7 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 79 reqs, GPU KV cache usage: 98.2%, CPU KV cache usage: 0.0%
total_padding=22984
input_metadata = InputMetadata(prompt_lens=[839, 723], num_prompts=2, max_context_len=None,)
total_padding=22984
input_metadata = InputMetadata(prompt_lens=[723], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[729, 730], num_prompts=2, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[733], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[734], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[746], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[752], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[752], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[754], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[762], num_prompts=1, max_context_len=None,)
total_padding=22985
input_metadata = InputMetadata(prompt_lens=[1290], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[762, 765], num_prompts=2, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[767], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[773], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[777], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[788], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[791], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[797], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[802], num_prompts=1, max_context_len=None,)
INFO 12-27 14:23:57 llm_engine.py:666] Avg prompt throughput: 3880.5 tokens/s, Avg generation throughput: 416.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 59 reqs, GPU KV cache usage: 97.3%, CPU KV cache usage: 0.0%
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[804], num_prompts=1, max_context_len=None,)
total_padding=22988
input_metadata = InputMetadata(prompt_lens=[807], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[807, 819], num_prompts=2, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[828], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[834], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[836], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[839], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[1445], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[840], num_prompts=1, max_context_len=None,)
INFO 12-27 14:24:02 llm_engine.py:666] Avg prompt throughput: 1775.0 tokens/s, Avg generation throughput: 501.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 50 reqs, GPU KV cache usage: 97.4%, CPU KV cache usage: 0.0%
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[846], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[1522], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[1570], num_prompts=1, max_context_len=None,)
INFO 12-27 14:24:07 llm_engine.py:666] Avg prompt throughput: 521.4 tokens/s, Avg generation throughput: 482.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 49 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 0.0%
promt select exit padding exceed_paddings=786, cur_paddings=0, exceeed_seq=848
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[1634], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[848], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[850], num_prompts=1, max_context_len=None,)
total_padding=23000
input_metadata = InputMetadata(prompt_lens=[856], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[857, 862], num_prompts=2, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[870], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[873], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[874], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[879], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[883], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[925], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[883], num_prompts=1, max_context_len=None,)
INFO 12-27 14:24:12 llm_engine.py:666] Avg prompt throughput: 2507.9 tokens/s, Avg generation throughput: 384.8 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 38 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 0.0%
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[886], num_prompts=1, max_context_len=None,)
total_padding=23005
input_metadata = InputMetadata(prompt_lens=[892], num_prompts=1, max_context_len=None,)
total_padding=23151
input_metadata = InputMetadata(prompt_lens=[1045, 899], num_prompts=2, max_context_len=None,)
total_padding=23157
input_metadata = InputMetadata(prompt_lens=[900, 906], num_prompts=2, max_context_len=None,)
total_padding=23159
input_metadata = InputMetadata(prompt_lens=[911, 913], num_prompts=2, max_context_len=None,)
total_padding=23161
input_metadata = InputMetadata(prompt_lens=[915, 917], num_prompts=2, max_context_len=None,)
total_padding=23161
input_metadata = InputMetadata(prompt_lens=[917], num_prompts=1, max_context_len=None,)
total_padding=23161
input_metadata = InputMetadata(prompt_lens=[918], num_prompts=1, max_context_len=None,)
INFO 12-27 14:24:17 llm_engine.py:666] Avg prompt throughput: 2130.2 tokens/s, Avg generation throughput: 388.5 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 27 reqs, GPU KV cache usage: 89.7%, CPU KV cache usage: 0.0%
total_padding=23161
input_metadata = InputMetadata(prompt_lens=[925], num_prompts=1, max_context_len=None,)
total_padding=23161
input_metadata = InputMetadata(prompt_lens=[927], num_prompts=1, max_context_len=None,)
total_padding=23164
input_metadata = InputMetadata(prompt_lens=[933, 936], num_prompts=2, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[937, 943], num_prompts=2, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[943], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[944], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[952], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[952], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[956], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[963], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[965], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[976], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[979], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[992], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[999], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1003], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1006], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1007], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1010], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1016], num_prompts=1, max_context_len=None,)
INFO 12-27 14:24:22 llm_engine.py:666] Avg prompt throughput: 4118.2 tokens/s, Avg generation throughput: 324.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 98.0%, CPU KV cache usage: 0.0%
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1017], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1017], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1022], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1023], num_prompts=1, max_context_len=None,)
total_padding=23170
input_metadata = InputMetadata(prompt_lens=[1024], num_prompts=1, max_context_len=None,)
INFO 12-27 14:24:27 llm_engine.py:666] Avg prompt throughput: 833.2 tokens/s, Avg generation throughput: 438.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 86.3%, CPU KV cache usage: 0.0%
INFO 12-27 14:24:32 llm_engine.py:666] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 203.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 20.8%, CPU KV cache usage: 0.0%
Throughput: 4.16 requests/s, 1991.42 tokens/s
