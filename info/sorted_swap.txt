Namespace(backend='vllm', dataset='/home/jiangjz/llm/TDoppeladler/dataset/ShareGPT_V3_unfiltered_cleaned_split.json', dtype='auto', enforce_eager=False, hf_max_batch_size=None, input_len=None, max_model_len=None, model='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', n=1, num_prompts=1000, output_len=None, quantization=None, seed=0, tensor_parallel_size=1, tokenizer='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', trust_remote_code=False, use_beam_search=False)
ModelConfig hidden_size=4096, head_size=128, swap_space_bytes=32, num_kv_heads=32        num_layers=32
CacheConfig block_size=16, gpu_memory_utilization=0.9, swap_space_bytes=34359738368,         sliding_window=None
SchedulerConfig max_num_batched_tokens=4096, max_model_len=4096, max_num_seqs=256,         max_paddings=256
INFO 12-28 15:48:45 llm_engine.py:74] Initializing an LLM engine with config: model='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', tokenizer='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
input_metadata = InputMetadata(prompt_lens=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16], num_prompts=256, max_context_len=None,)
INFO 12-28 15:48:51 llm_engine.py:236] # GPU blocks: 954, # CPU blocks: 4096
INFO 12-28 15:48:57 model_runner.py:403] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-28 15:48:57 model_runner.py:407] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
INFO 12-28 15:48:59 model_runner.py:449] Graph capturing finished in 2 secs.
promt select exit padding exceed_paddings=301, cur_paddings=210, exceeed_seq=11
input_metadata = InputMetadata(prompt_lens=[4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], num_prompts=91, max_context_len=None,)
promt select exit padding exceed_paddings=279, cur_paddings=182, exceeed_seq=16
input_metadata = InputMetadata(prompt_lens=[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15], num_prompts=97, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21], num_prompts=68, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[21], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[22, 22], num_prompts=2, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[22], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[23], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[23, 23, 23], num_prompts=3, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[23, 23], num_prompts=2, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[24], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
input_metadata = InputMetadata(prompt_lens=[25], num_prompts=1, max_context_len=None,)
promt select exit seqs total_seqs=257, num_curr_seqs=256
promt select exit seqs total_seqs=257, num_curr_seqs=256
INFO 12-28 15:49:04 llm_engine.py:693] Avg past 5 seconds prompt throughput: 922.8 tokens/s, Avg past 5 seconds real prompt throughput: 793.6 tokens/s, Avg past 5 seconds generation throughput: 4705.2 tokens/s, total prompt throughput: 4243.0 tokens/s, total real prompt throughput: 3649.0 tokens/s, total final prompt throughput: 3649.0 tokens/s, total generation throughput: 20040.0 tokens/s, avg total prompt throughput: 922.8 tokens/s, avg total real prompt throughput: 793.6 tokens/s, avg total final prompt throughput: 793.6 tokens/s, avg total generation throughput: 4705.2 tokens/s, Running: 132 reqs, Swapped: 92 reqs, Waiting: 723 reqs, GPU KV cache usage: 97.7%, CPU KV cache usage: 11.0%
INFO 12-28 15:49:09 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 2635.2 tokens/s, total prompt throughput: 4243.0 tokens/s, total real prompt throughput: 3649.0 tokens/s, total final prompt throughput: 3649.0 tokens/s, total generation throughput: 33292.0 tokens/s, avg total prompt throughput: 441.4 tokens/s, avg total real prompt throughput: 379.6 tokens/s, avg total final prompt throughput: 379.6 tokens/s, avg total generation throughput: 3589.6 tokens/s, Running: 56 reqs, Swapped: 133 reqs, Waiting: 723 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 20.6%
INFO 12-28 15:49:14 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1625.5 tokens/s, total prompt throughput: 4243.0 tokens/s, total real prompt throughput: 3649.0 tokens/s, total final prompt throughput: 3649.0 tokens/s, total generation throughput: 41443.0 tokens/s, avg total prompt throughput: 290.0 tokens/s, avg total real prompt throughput: 249.4 tokens/s, avg total final prompt throughput: 249.4 tokens/s, avg total generation throughput: 2900.1 tokens/s, Running: 46 reqs, Swapped: 101 reqs, Waiting: 723 reqs, GPU KV cache usage: 98.6%, CPU KV cache usage: 12.7%
INFO 12-28 15:49:19 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1550.1 tokens/s, total prompt throughput: 4243.0 tokens/s, total real prompt throughput: 3649.0 tokens/s, total final prompt throughput: 3649.0 tokens/s, total generation throughput: 49211.0 tokens/s, avg total prompt throughput: 216.0 tokens/s, avg total real prompt throughput: 185.8 tokens/s, avg total final prompt throughput: 185.8 tokens/s, avg total generation throughput: 2549.8 tokens/s, Running: 43 reqs, Swapped: 73 reqs, Waiting: 723 reqs, GPU KV cache usage: 99.2%, CPU KV cache usage: 8.1%
INFO 12-28 15:49:24 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1200.2 tokens/s, total prompt throughput: 4243.0 tokens/s, total real prompt throughput: 3649.0 tokens/s, total final prompt throughput: 3649.0 tokens/s, total generation throughput: 55238.0 tokens/s, avg total prompt throughput: 172.1 tokens/s, avg total real prompt throughput: 148.0 tokens/s, avg total final prompt throughput: 148.0 tokens/s, avg total generation throughput: 2271.8 tokens/s, Running: 34 reqs, Swapped: 63 reqs, Waiting: 723 reqs, GPU KV cache usage: 95.2%, CPU KV cache usage: 6.7%
INFO 12-28 15:49:29 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1504.7 tokens/s, total prompt throughput: 4243.0 tokens/s, total real prompt throughput: 3649.0 tokens/s, total final prompt throughput: 3649.0 tokens/s, total generation throughput: 62747.0 tokens/s, avg total prompt throughput: 143.0 tokens/s, avg total real prompt throughput: 123.0 tokens/s, avg total final prompt throughput: 123.0 tokens/s, avg total generation throughput: 2139.9 tokens/s, Running: 51 reqs, Swapped: 21 reqs, Waiting: 723 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 2.1%
input_metadata = InputMetadata(prompt_lens=[25, 25, 25], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[25, 25], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[25, 25], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[25, 25, 25, 26, 26, 26, 26, 26], num_prompts=8, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[26, 26, 26], num_prompts=3, max_context_len=None,)
INFO 12-28 15:49:34 llm_engine.py:693] Avg past 5 seconds prompt throughput: 84.5 tokens/s, Avg past 5 seconds real prompt throughput: 83.8 tokens/s, Avg past 5 seconds generation throughput: 1677.6 tokens/s, total prompt throughput: 4651.0 tokens/s, total real prompt throughput: 4054.0 tokens/s, total final prompt throughput: 4054.0 tokens/s, total generation throughput: 71150.0 tokens/s, avg total prompt throughput: 134.1 tokens/s, avg total real prompt throughput: 116.9 tokens/s, avg total final prompt throughput: 116.9 tokens/s, avg total generation throughput: 2072.4 tokens/s, Running: 50 reqs, Swapped: 4 reqs, Waiting: 705 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.5%
input_metadata = InputMetadata(prompt_lens=[26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29], num_prompts=25, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30], num_prompts=11, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[31, 31, 31, 31, 31], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[31, 32, 32, 32, 32, 33, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35], num_prompts=20, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[35, 36], num_prompts=2, max_context_len=None,)
INFO 12-28 15:49:39 llm_engine.py:693] Avg past 5 seconds prompt throughput: 401.0 tokens/s, Avg past 5 seconds real prompt throughput: 385.1 tokens/s, Avg past 5 seconds generation throughput: 2171.2 tokens/s, total prompt throughput: 6639.0 tokens/s, total real prompt throughput: 5966.0 tokens/s, total final prompt throughput: 5966.0 tokens/s, total generation throughput: 82048.0 tokens/s, avg total prompt throughput: 167.2 tokens/s, avg total real prompt throughput: 150.3 tokens/s, avg total final prompt throughput: 150.3 tokens/s, avg total generation throughput: 2084.5 tokens/s, Running: 70 reqs, Swapped: 12 reqs, Waiting: 642 reqs, GPU KV cache usage: 97.2%, CPU KV cache usage: 2.3%
INFO 12-28 15:49:44 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1770.9 tokens/s, total prompt throughput: 6639.0 tokens/s, total real prompt throughput: 5966.0 tokens/s, total final prompt throughput: 5966.0 tokens/s, total generation throughput: 90952.0 tokens/s, avg total prompt throughput: 148.4 tokens/s, avg total real prompt throughput: 133.4 tokens/s, avg total final prompt throughput: 133.4 tokens/s, avg total generation throughput: 2049.2 tokens/s, Running: 39 reqs, Swapped: 24 reqs, Waiting: 642 reqs, GPU KV cache usage: 92.1%, CPU KV cache usage: 8.1%
input_metadata = InputMetadata(prompt_lens=[36], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[36], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[36], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[36, 36, 37], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39], num_prompts=13, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[39, 40, 40, 40, 41, 41], num_prompts=6, max_context_len=None,)
INFO 12-28 15:49:49 llm_engine.py:693] Avg past 5 seconds prompt throughput: 233.8 tokens/s, Avg past 5 seconds real prompt throughput: 229.0 tokens/s, Avg past 5 seconds generation throughput: 1409.8 tokens/s, total prompt throughput: 7437.0 tokens/s, total real prompt throughput: 6748.0 tokens/s, total final prompt throughput: 6748.0 tokens/s, total generation throughput: 97995.0 tokens/s, avg total prompt throughput: 149.5 tokens/s, avg total real prompt throughput: 135.7 tokens/s, avg total final prompt throughput: 135.7 tokens/s, avg total generation throughput: 1984.0 tokens/s, Running: 50 reqs, Swapped: 2 reqs, Waiting: 617 reqs, GPU KV cache usage: 99.6%, CPU KV cache usage: 0.4%
input_metadata = InputMetadata(prompt_lens=[41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 44, 44, 44, 44], num_prompts=14, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[44, 45], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[45, 45, 46, 46, 47, 47, 47, 48, 49, 49, 49, 50], num_prompts=12, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[50, 50], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[50, 50, 51, 52, 53, 53, 54, 55, 55], num_prompts=9, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[55], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[56, 56, 57, 57, 57, 57], num_prompts=6, max_context_len=None,)
INFO 12-28 15:49:54 llm_engine.py:693] Avg past 5 seconds prompt throughput: 679.6 tokens/s, Avg past 5 seconds real prompt throughput: 652.8 tokens/s, Avg past 5 seconds generation throughput: 1695.7 tokens/s, total prompt throughput: 9639.0 tokens/s, total real prompt throughput: 8868.0 tokens/s, total final prompt throughput: 8868.0 tokens/s, total generation throughput: 106530.0 tokens/s, avg total prompt throughput: 176.0 tokens/s, avg total real prompt throughput: 161.9 tokens/s, avg total final prompt throughput: 161.9 tokens/s, avg total generation throughput: 1957.2 tokens/s, Running: 67 reqs, Swapped: 6 reqs, Waiting: 571 reqs, GPU KV cache usage: 96.8%, CPU KV cache usage: 1.7%
INFO 12-28 15:49:59 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1651.5 tokens/s, total prompt throughput: 9639.0 tokens/s, total real prompt throughput: 8868.0 tokens/s, total final prompt throughput: 8868.0 tokens/s, total generation throughput: 114836.0 tokens/s, avg total prompt throughput: 161.2 tokens/s, avg total real prompt throughput: 148.3 tokens/s, avg total final prompt throughput: 148.3 tokens/s, avg total generation throughput: 1931.7 tokens/s, Running: 40 reqs, Swapped: 16 reqs, Waiting: 571 reqs, GPU KV cache usage: 99.0%, CPU KV cache usage: 4.5%
input_metadata = InputMetadata(prompt_lens=[57], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[57], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[58, 58], num_prompts=2, max_context_len=None,)
INFO 12-28 15:50:04 llm_engine.py:693] Avg past 5 seconds prompt throughput: 96.1 tokens/s, Avg past 5 seconds real prompt throughput: 96.1 tokens/s, Avg past 5 seconds generation throughput: 1196.7 tokens/s, total prompt throughput: 10095.0 tokens/s, total real prompt throughput: 9322.0 tokens/s, total final prompt throughput: 9322.0 tokens/s, total generation throughput: 120843.0 tokens/s, avg total prompt throughput: 155.8 tokens/s, avg total real prompt throughput: 143.9 tokens/s, avg total final prompt throughput: 143.9 tokens/s, avg total generation throughput: 1874.6 tokens/s, Running: 34 reqs, Swapped: 2 reqs, Waiting: 567 reqs, GPU KV cache usage: 98.5%, CPU KV cache usage: 0.5%
input_metadata = InputMetadata(prompt_lens=[58, 58, 59, 59, 59, 61, 62], num_prompts=7, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[62, 62, 63, 63, 64, 64, 66, 66, 66, 67, 67, 68, 68], num_prompts=13, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[72, 72, 73, 73, 74, 76, 77, 77, 78], num_prompts=9, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[78, 79, 79, 79, 80, 80, 81, 81, 82], num_prompts=9, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[82, 83, 83], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[83, 84], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[84], num_prompts=1, max_context_len=None,)
INFO 12-28 15:50:09 llm_engine.py:693] Avg past 5 seconds prompt throughput: 736.8 tokens/s, Avg past 5 seconds real prompt throughput: 711.9 tokens/s, Avg past 5 seconds generation throughput: 1566.1 tokens/s, total prompt throughput: 13386.0 tokens/s, total real prompt throughput: 12506.0 tokens/s, total final prompt throughput: 12506.0 tokens/s, total generation throughput: 128664.0 tokens/s, avg total prompt throughput: 191.8 tokens/s, avg total real prompt throughput: 179.2 tokens/s, avg total final prompt throughput: 179.2 tokens/s, avg total generation throughput: 1852.3 tokens/s, Running: 47 reqs, Swapped: 4 reqs, Waiting: 523 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 1.0%
INFO 12-28 15:50:14 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1287.0 tokens/s, total prompt throughput: 13386.0 tokens/s, total real prompt throughput: 12506.0 tokens/s, total final prompt throughput: 12506.0 tokens/s, total generation throughput: 135143.0 tokens/s, avg total prompt throughput: 178.9 tokens/s, avg total real prompt throughput: 167.1 tokens/s, avg total final prompt throughput: 167.1 tokens/s, avg total generation throughput: 1814.2 tokens/s, Running: 33 reqs, Swapped: 8 reqs, Waiting: 523 reqs, GPU KV cache usage: 96.8%, CPU KV cache usage: 3.4%
input_metadata = InputMetadata(prompt_lens=[85], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[85], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[85, 86, 87, 87, 88, 88, 88, 89], num_prompts=8, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[89], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[89, 89, 90, 90], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[92, 92, 92, 93], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[94, 94, 95], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[95, 96, 96], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[97, 97], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[99, 99, 99, 99], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[100], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[101, 101], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[101], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[101, 103, 104], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[105], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[106, 107, 107], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[108, 108, 109, 109, 112, 112], num_prompts=6, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[114, 115, 118, 119], num_prompts=4, max_context_len=None,)
INFO 12-28 15:50:19 llm_engine.py:693] Avg past 5 seconds prompt throughput: 992.5 tokens/s, Avg past 5 seconds real prompt throughput: 983.8 tokens/s, Avg past 5 seconds generation throughput: 1270.8 tokens/s, total prompt throughput: 18149.0 tokens/s, total real prompt throughput: 17228.0 tokens/s, total final prompt throughput: 17228.0 tokens/s, total generation throughput: 141500.0 tokens/s, avg total prompt throughput: 227.3 tokens/s, avg total real prompt throughput: 215.8 tokens/s, avg total final prompt throughput: 215.8 tokens/s, avg total generation throughput: 1780.0 tokens/s, Running: 58 reqs, Swapped: 2 reqs, Waiting: 471 reqs, GPU KV cache usage: 99.0%, CPU KV cache usage: 0.5%
INFO 12-28 15:50:24 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1469.0 tokens/s, total prompt throughput: 18149.0 tokens/s, total real prompt throughput: 17228.0 tokens/s, total final prompt throughput: 17228.0 tokens/s, total generation throughput: 148878.0 tokens/s, avg total prompt throughput: 213.9 tokens/s, avg total real prompt throughput: 203.0 tokens/s, avg total final prompt throughput: 203.0 tokens/s, avg total generation throughput: 1761.7 tokens/s, Running: 35 reqs, Swapped: 7 reqs, Waiting: 471 reqs, GPU KV cache usage: 95.7%, CPU KV cache usage: 3.9%
input_metadata = InputMetadata(prompt_lens=[121, 121], num_prompts=2, max_context_len=None,)
INFO 12-28 15:50:29 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1101.2 tokens/s, total prompt throughput: 18625.0 tokens/s, total real prompt throughput: 17694.0 tokens/s, total final prompt throughput: 17694.0 tokens/s, total generation throughput: 154410.0 tokens/s, avg total prompt throughput: 207.3 tokens/s, avg total real prompt throughput: 196.9 tokens/s, avg total final prompt throughput: 196.9 tokens/s, avg total generation throughput: 1724.7 tokens/s, Running: 26 reqs, Swapped: 4 reqs, Waiting: 469 reqs, GPU KV cache usage: 98.4%, CPU KV cache usage: 1.3%
input_metadata = InputMetadata(prompt_lens=[121], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[122], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[123, 123, 124], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[126, 126, 131, 131, 131, 131, 131, 132, 133], num_prompts=9, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[134], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[135, 136, 136, 137, 137, 138], num_prompts=6, max_context_len=None,)
INFO 12-28 15:50:34 llm_engine.py:693] Avg past 5 seconds prompt throughput: 426.4 tokens/s, Avg past 5 seconds real prompt throughput: 420.5 tokens/s, Avg past 5 seconds generation throughput: 900.7 tokens/s, total prompt throughput: 20813.0 tokens/s, total real prompt throughput: 19855.0 tokens/s, total final prompt throughput: 19855.0 tokens/s, total generation throughput: 158935.0 tokens/s, avg total prompt throughput: 219.3 tokens/s, avg total real prompt throughput: 209.2 tokens/s, avg total final prompt throughput: 209.2 tokens/s, avg total generation throughput: 1681.0 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Waiting: 448 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[138, 140, 142, 143], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[144, 146, 147], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[148, 148, 149], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[150, 151, 152, 156, 159, 159, 164], num_prompts=7, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[164, 164, 166, 167, 167, 167, 167, 167, 167], num_prompts=9, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[167, 168], num_prompts=2, max_context_len=None,)
INFO 12-28 15:50:39 llm_engine.py:693] Avg past 5 seconds prompt throughput: 858.5 tokens/s, Avg past 5 seconds real prompt throughput: 842.0 tokens/s, Avg past 5 seconds generation throughput: 1273.6 tokens/s, total prompt throughput: 25752.0 tokens/s, total real prompt throughput: 24706.0 tokens/s, total final prompt throughput: 24706.0 tokens/s, total generation throughput: 165330.0 tokens/s, avg total prompt throughput: 257.7 tokens/s, avg total real prompt throughput: 247.3 tokens/s, avg total final prompt throughput: 247.3 tokens/s, avg total generation throughput: 1660.4 tokens/s, Running: 39 reqs, Swapped: 4 reqs, Waiting: 420 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 1.6%
input_metadata = InputMetadata(prompt_lens=[168], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[168, 171], num_prompts=2, max_context_len=None,)
INFO 12-28 15:50:44 llm_engine.py:693] Avg past 5 seconds prompt throughput: 43.7 tokens/s, Avg past 5 seconds real prompt throughput: 43.7 tokens/s, Avg past 5 seconds generation throughput: 1177.6 tokens/s, total prompt throughput: 26256.0 tokens/s, total real prompt throughput: 25209.0 tokens/s, total final prompt throughput: 25209.0 tokens/s, total generation throughput: 171236.0 tokens/s, avg total prompt throughput: 250.2 tokens/s, avg total real prompt throughput: 240.2 tokens/s, avg total final prompt throughput: 240.2 tokens/s, avg total generation throughput: 1637.2 tokens/s, Running: 28 reqs, Swapped: 4 reqs, Waiting: 417 reqs, GPU KV cache usage: 92.3%, CPU KV cache usage: 2.0%
input_metadata = InputMetadata(prompt_lens=[171, 175], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[175, 175, 178, 178], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[179, 180, 180, 181], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[182], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[183, 184], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[184], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[185, 187, 189], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[189, 191], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[192, 194], num_prompts=2, max_context_len=None,)
INFO 12-28 15:50:49 llm_engine.py:693] Avg past 5 seconds prompt throughput: 728.0 tokens/s, Avg past 5 seconds real prompt throughput: 723.1 tokens/s, Avg past 5 seconds generation throughput: 1061.1 tokens/s, total prompt throughput: 30067.0 tokens/s, total real prompt throughput: 28994.0 tokens/s, total final prompt throughput: 28994.0 tokens/s, total generation throughput: 176519.0 tokens/s, avg total prompt throughput: 273.5 tokens/s, avg total real prompt throughput: 263.7 tokens/s, avg total final prompt throughput: 263.7 tokens/s, avg total generation throughput: 1610.6 tokens/s, Running: 30 reqs, Swapped: 3 reqs, Waiting: 396 reqs, GPU KV cache usage: 99.3%, CPU KV cache usage: 1.4%
input_metadata = InputMetadata(prompt_lens=[194, 194, 197, 197, 201], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[202], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[204, 206], num_prompts=2, max_context_len=None,)
INFO 12-28 15:50:54 llm_engine.py:693] Avg past 5 seconds prompt throughput: 473.1 tokens/s, Avg past 5 seconds real prompt throughput: 464.5 tokens/s, Avg past 5 seconds generation throughput: 917.6 tokens/s, total prompt throughput: 31662.0 tokens/s, total real prompt throughput: 30565.0 tokens/s, total final prompt throughput: 30565.0 tokens/s, total generation throughput: 181127.0 tokens/s, avg total prompt throughput: 275.4 tokens/s, avg total real prompt throughput: 265.9 tokens/s, avg total final prompt throughput: 265.9 tokens/s, avg total generation throughput: 1580.3 tokens/s, Running: 25 reqs, Swapped: 2 reqs, Waiting: 388 reqs, GPU KV cache usage: 98.2%, CPU KV cache usage: 1.0%
input_metadata = InputMetadata(prompt_lens=[209, 210, 210], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[210, 211, 211, 218], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[219], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[220, 223, 224], num_prompts=3, max_context_len=None,)
INFO 12-28 15:50:59 llm_engine.py:693] Avg past 5 seconds prompt throughput: 462.7 tokens/s, Avg past 5 seconds real prompt throughput: 456.5 tokens/s, Avg past 5 seconds generation throughput: 785.4 tokens/s, total prompt throughput: 33795.0 tokens/s, total real prompt throughput: 32673.0 tokens/s, total final prompt throughput: 32673.0 tokens/s, total generation throughput: 185069.0 tokens/s, avg total prompt throughput: 281.7 tokens/s, avg total real prompt throughput: 272.3 tokens/s, avg total final prompt throughput: 272.3 tokens/s, avg total generation throughput: 1547.0 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Waiting: 377 reqs, GPU KV cache usage: 98.3%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[226, 228], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[228], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[232, 232], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[232, 233, 234, 234], num_prompts=4, max_context_len=None,)
INFO 12-28 15:51:04 llm_engine.py:693] Avg past 5 seconds prompt throughput: 252.3 tokens/s, Avg past 5 seconds real prompt throughput: 251.9 tokens/s, Avg past 5 seconds generation throughput: 822.0 tokens/s, total prompt throughput: 35615.0 tokens/s, total real prompt throughput: 34486.0 tokens/s, total final prompt throughput: 34486.0 tokens/s, total generation throughput: 189186.0 tokens/s, avg total prompt throughput: 285.0 tokens/s, avg total real prompt throughput: 275.9 tokens/s, avg total final prompt throughput: 275.9 tokens/s, avg total generation throughput: 1517.9 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Waiting: 368 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[235, 236, 236, 240], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[243, 243, 244, 245], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[245], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[245], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[248], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[248], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[249], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[249, 253, 255], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[256, 259], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[262], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[262], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[263, 264], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[269], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[271, 274, 276], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[277], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[278], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[282], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[282, 283], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[285], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[286], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[288], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[291], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[294], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[301], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[301], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[301, 302], num_prompts=2, max_context_len=None,)
INFO 12-28 15:51:09 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2049.3 tokens/s, Avg past 5 seconds real prompt throughput: 2041.6 tokens/s, Avg past 5 seconds generation throughput: 823.2 tokens/s, total prompt throughput: 46607.0 tokens/s, total real prompt throughput: 45437.0 tokens/s, total final prompt throughput: 45437.0 tokens/s, total generation throughput: 193326.0 tokens/s, avg total prompt throughput: 358.5 tokens/s, avg total real prompt throughput: 349.5 tokens/s, avg total final prompt throughput: 349.5 tokens/s, avg total generation throughput: 1491.0 tokens/s, Running: 28 reqs, Swapped: 1 reqs, Waiting: 328 reqs, GPU KV cache usage: 97.7%, CPU KV cache usage: 0.9%
input_metadata = InputMetadata(prompt_lens=[303], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[304], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[304], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[306], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[307], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[308], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[308], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[309], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[314, 314], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[316], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[320], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[320], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:14 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1100.4 tokens/s, Avg past 5 seconds real prompt throughput: 1100.4 tokens/s, Avg past 5 seconds generation throughput: 840.4 tokens/s, total prompt throughput: 52474.0 tokens/s, total real prompt throughput: 51303.0 tokens/s, total final prompt throughput: 51303.0 tokens/s, total generation throughput: 197552.0 tokens/s, avg total prompt throughput: 388.6 tokens/s, avg total real prompt throughput: 380.0 tokens/s, avg total final prompt throughput: 380.0 tokens/s, avg total generation throughput: 1466.8 tokens/s, Running: 22 reqs, Swapped: 1 reqs, Waiting: 310 reqs, GPU KV cache usage: 97.4%, CPU KV cache usage: 0.7%
input_metadata = InputMetadata(prompt_lens=[322, 323, 323], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[325], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[325], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[328], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[329], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[329], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[329], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[330], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[331], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[332, 333], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[337, 338, 338, 341], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[342], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[342], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[344], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[344], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[348, 348], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[349], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[350], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[352], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:19 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2176.4 tokens/s, Avg past 5 seconds real prompt throughput: 2173.3 tokens/s, Avg past 5 seconds generation throughput: 696.5 tokens/s, total prompt throughput: 61186.0 tokens/s, total real prompt throughput: 60003.0 tokens/s, total final prompt throughput: 60003.0 tokens/s, total generation throughput: 201067.0 tokens/s, avg total prompt throughput: 436.9 tokens/s, avg total real prompt throughput: 428.4 tokens/s, avg total final prompt throughput: 428.4 tokens/s, avg total generation throughput: 1439.1 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Waiting: 284 reqs, GPU KV cache usage: 99.1%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[353], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[355, 356], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[358], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[358], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[360], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[364], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[365, 366], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[366], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[367, 369], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[370], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[373, 373, 375], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[376], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[377], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[378], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[380], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[381], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[382, 382, 383, 384], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[385], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[387], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[387, 387], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[390], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[390], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[391], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[392], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[397, 397], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[397], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[399], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[404], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[406], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:24 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2906.5 tokens/s, Avg past 5 seconds real prompt throughput: 2903.8 tokens/s, Avg past 5 seconds generation throughput: 614.4 tokens/s, total prompt throughput: 75905.0 tokens/s, total real prompt throughput: 74709.0 tokens/s, total final prompt throughput: 74709.0 tokens/s, total generation throughput: 204154.0 tokens/s, avg total prompt throughput: 523.2 tokens/s, avg total real prompt throughput: 515.0 tokens/s, avg total final prompt throughput: 515.0 tokens/s, avg total generation throughput: 1410.6 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Waiting: 245 reqs, GPU KV cache usage: 98.8%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[408, 408], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[410], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[413], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[413], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[413], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[415, 415], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[415], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[418], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[419], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[421], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[421], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:29 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1071.0 tokens/s, Avg past 5 seconds real prompt throughput: 1071.0 tokens/s, Avg past 5 seconds generation throughput: 786.9 tokens/s, total prompt throughput: 81279.0 tokens/s, total real prompt throughput: 80083.0 tokens/s, total final prompt throughput: 80083.0 tokens/s, total generation throughput: 208097.0 tokens/s, avg total prompt throughput: 541.6 tokens/s, avg total real prompt throughput: 533.6 tokens/s, avg total final prompt throughput: 533.6 tokens/s, avg total generation throughput: 1389.8 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Waiting: 232 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[422, 423], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[424, 425], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[427], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[432], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[435, 438], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[438], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[439], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[439], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[441], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[444], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[445, 445, 445], num_prompts=3, max_context_len=None,)
INFO 12-28 15:51:34 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1644.3 tokens/s, Avg past 5 seconds real prompt throughput: 1643.2 tokens/s, Avg past 5 seconds generation throughput: 617.6 tokens/s, total prompt throughput: 89106.0 tokens/s, total real prompt throughput: 87905.0 tokens/s, total final prompt throughput: 87905.0 tokens/s, total generation throughput: 211184.0 tokens/s, avg total prompt throughput: 574.6 tokens/s, avg total real prompt throughput: 566.8 tokens/s, avg total final prompt throughput: 566.8 tokens/s, avg total generation throughput: 1364.8 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Waiting: 212 reqs, GPU KV cache usage: 97.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[447], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[451], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[451], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[452, 452], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[457], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[457], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[458], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[458], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[458], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[459, 460, 460, 461, 461], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[461], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[463], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[464], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[465], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[468], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[468, 468, 470], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[471], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[475], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[475, 476], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[481], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[482], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[487, 489], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[490], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[493], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:39 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3159.1 tokens/s, Avg past 5 seconds real prompt throughput: 3156.8 tokens/s, Avg past 5 seconds generation throughput: 587.5 tokens/s, total prompt throughput: 105792.0 tokens/s, total real prompt throughput: 104580.0 tokens/s, total final prompt throughput: 104580.0 tokens/s, total generation throughput: 214134.0 tokens/s, avg total prompt throughput: 660.8 tokens/s, avg total real prompt throughput: 653.3 tokens/s, avg total final prompt throughput: 653.3 tokens/s, avg total generation throughput: 1340.4 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Waiting: 178 reqs, GPU KV cache usage: 97.0%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[496], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[498, 498], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[500], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[501], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[504], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[504], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[505], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[506], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[509], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[510], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[512], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[517, 518], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[519], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[520], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[521], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[523], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[523], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[525], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[529], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[534, 536], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[536], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:44 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2766.5 tokens/s, Avg past 5 seconds real prompt throughput: 2765.8 tokens/s, Avg past 5 seconds generation throughput: 610.7 tokens/s, total prompt throughput: 119692.0 tokens/s, total real prompt throughput: 118477.0 tokens/s, total final prompt throughput: 118477.0 tokens/s, total generation throughput: 217210.0 tokens/s, avg total prompt throughput: 724.9 tokens/s, avg total real prompt throughput: 717.5 tokens/s, avg total final prompt throughput: 717.5 tokens/s, avg total generation throughput: 1318.2 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Waiting: 151 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[538], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[539], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[540], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[541], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[543], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[549], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[550], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[551], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[554], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[554], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[558], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[559], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[560], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[560], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[563], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[564], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[570], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[578], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:49 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2045.8 tokens/s, Avg past 5 seconds real prompt throughput: 2045.8 tokens/s, Avg past 5 seconds generation throughput: 608.0 tokens/s, total prompt throughput: 129621.0 tokens/s, total real prompt throughput: 128406.0 tokens/s, total final prompt throughput: 128406.0 tokens/s, total generation throughput: 220258.0 tokens/s, avg total prompt throughput: 761.9 tokens/s, avg total real prompt throughput: 754.7 tokens/s, avg total final prompt throughput: 754.7 tokens/s, avg total generation throughput: 1297.2 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Waiting: 133 reqs, GPU KV cache usage: 96.4%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[580], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[580], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[584], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[585], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[587], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[593, 596], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[597], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[598], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[599], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[601], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[604], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[609], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:54 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1450.5 tokens/s, Avg past 5 seconds real prompt throughput: 1449.8 tokens/s, Avg past 5 seconds generation throughput: 587.3 tokens/s, total prompt throughput: 137306.0 tokens/s, total real prompt throughput: 136088.0 tokens/s, total final prompt throughput: 136088.0 tokens/s, total generation throughput: 223206.0 tokens/s, avg total prompt throughput: 783.9 tokens/s, avg total real prompt throughput: 777.0 tokens/s, avg total final prompt throughput: 777.0 tokens/s, avg total generation throughput: 1276.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Waiting: 120 reqs, GPU KV cache usage: 97.6%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[609, 611], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[612], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[613], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[616], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[616], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[622], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[624, 627], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[627], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[629], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[633], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[637], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[652, 656], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[658], num_prompts=1, max_context_len=None,)
INFO 12-28 15:51:59 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1994.7 tokens/s, Avg past 5 seconds real prompt throughput: 1992.8 tokens/s, Avg past 5 seconds generation throughput: 513.4 tokens/s, total prompt throughput: 147308.0 tokens/s, total real prompt throughput: 146081.0 tokens/s, total final prompt throughput: 146081.0 tokens/s, total generation throughput: 225779.0 tokens/s, avg total prompt throughput: 817.7 tokens/s, avg total real prompt throughput: 810.9 tokens/s, avg total final prompt throughput: 810.9 tokens/s, avg total generation throughput: 1255.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Waiting: 104 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[661], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[662], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[664], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[665], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[668], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[668, 672], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[672, 673], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[673], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[675], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[678], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[682], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[684], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[686], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[686], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[686], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[690], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[692, 696], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[697], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[701, 703], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[706], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[719], num_prompts=1, max_context_len=None,)
INFO 12-28 15:52:04 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3370.8 tokens/s, Avg past 5 seconds real prompt throughput: 3368.5 tokens/s, Avg past 5 seconds generation throughput: 409.6 tokens/s, total prompt throughput: 164317.0 tokens/s, total real prompt throughput: 163079.0 tokens/s, total final prompt throughput: 163079.0 tokens/s, total generation throughput: 227836.0 tokens/s, avg total prompt throughput: 887.4 tokens/s, avg total real prompt throughput: 880.7 tokens/s, avg total final prompt throughput: 880.7 tokens/s, avg total generation throughput: 1232.7 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Waiting: 79 reqs, GPU KV cache usage: 97.5%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[723, 723], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[729], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[730], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[733, 734], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[746, 752], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[752], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[754], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[762], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[762, 765], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[767], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[773], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[777, 788], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[791], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[797], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[802], num_prompts=1, max_context_len=None,)
INFO 12-28 15:52:09 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3913.3 tokens/s, Avg past 5 seconds real prompt throughput: 3907.6 tokens/s, Avg past 5 seconds generation throughput: 445.8 tokens/s, total prompt throughput: 179415.0 tokens/s, total real prompt throughput: 178156.0 tokens/s, total final prompt throughput: 178156.0 tokens/s, total generation throughput: 230107.0 tokens/s, avg total prompt throughput: 943.2 tokens/s, avg total real prompt throughput: 936.6 tokens/s, avg total final prompt throughput: 936.6 tokens/s, avg total generation throughput: 1211.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Waiting: 59 reqs, GPU KV cache usage: 97.2%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[804], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[807], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[807, 819], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[828], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[834], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[836], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[839], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[840], num_prompts=1, max_context_len=None,)
INFO 12-28 15:52:14 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1449.8 tokens/s, Avg past 5 seconds real prompt throughput: 1447.2 tokens/s, Avg past 5 seconds generation throughput: 502.4 tokens/s, total prompt throughput: 186803.0 tokens/s, total real prompt throughput: 185532.0 tokens/s, total final prompt throughput: 185532.0 tokens/s, total generation throughput: 232633.0 tokens/s, avg total prompt throughput: 956.8 tokens/s, avg total real prompt throughput: 950.3 tokens/s, avg total final prompt throughput: 950.3 tokens/s, avg total generation throughput: 1193.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Waiting: 50 reqs, GPU KV cache usage: 97.8%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[846], num_prompts=1, max_context_len=None,)
INFO 12-28 15:52:19 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 489.5 tokens/s, total prompt throughput: 187643.0 tokens/s, total real prompt throughput: 186372.0 tokens/s, total final prompt throughput: 186372.0 tokens/s, total generation throughput: 235092.0 tokens/s, avg total prompt throughput: 937.0 tokens/s, avg total real prompt throughput: 930.7 tokens/s, avg total final prompt throughput: 930.7 tokens/s, avg total generation throughput: 1175.9 tokens/s, Running: 11 reqs, Swapped: 1 reqs, Waiting: 49 reqs, GPU KV cache usage: 89.4%, CPU KV cache usage: 2.5%
input_metadata = InputMetadata(prompt_lens=[848], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[850], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[856, 857], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[862], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[870], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[873], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[874], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[879], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[883], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[883], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[886], num_prompts=1, max_context_len=None,)
INFO 12-28 15:52:24 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2008.3 tokens/s, Avg past 5 seconds real prompt throughput: 2008.0 tokens/s, Avg past 5 seconds generation throughput: 392.4 tokens/s, total prompt throughput: 198025.0 tokens/s, total real prompt throughput: 196753.0 tokens/s, total final prompt throughput: 196753.0 tokens/s, total generation throughput: 237064.0 tokens/s, avg total prompt throughput: 964.7 tokens/s, avg total real prompt throughput: 958.5 tokens/s, avg total final prompt throughput: 958.5 tokens/s, avg total generation throughput: 1156.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Waiting: 37 reqs, GPU KV cache usage: 96.1%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[892, 899], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[900], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[906, 911], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[913], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[915], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[917], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[917], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[918], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[925], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[927], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[933], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[936, 937], num_prompts=2, max_context_len=None,)
INFO 12-28 15:52:30 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3473.7 tokens/s, Avg past 5 seconds real prompt throughput: 3470.2 tokens/s, Avg past 5 seconds generation throughput: 359.9 tokens/s, total prompt throughput: 210796.0 tokens/s, total real prompt throughput: 209512.0 tokens/s, total final prompt throughput: 209512.0 tokens/s, total generation throughput: 238953.0 tokens/s, avg total prompt throughput: 1001.6 tokens/s, avg total real prompt throughput: 995.5 tokens/s, avg total final prompt throughput: 995.5 tokens/s, avg total generation throughput: 1137.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Waiting: 22 reqs, GPU KV cache usage: 94.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[943, 943], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[944], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[952], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[952], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[956], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[963], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[965], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[976], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[979], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[992], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[999], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1003], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1006], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1007], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1010], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1016], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1017, 1017], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1022], num_prompts=1, max_context_len=None,)
INFO 12-28 15:52:35 llm_engine.py:693] Avg past 5 seconds prompt throughput: 4058.4 tokens/s, Avg past 5 seconds real prompt throughput: 4058.4 tokens/s, Avg past 5 seconds generation throughput: 338.6 tokens/s, total prompt throughput: 231310.0 tokens/s, total real prompt throughput: 230025.0 tokens/s, total final prompt throughput: 230025.0 tokens/s, total generation throughput: 240656.0 tokens/s, avg total prompt throughput: 1073.5 tokens/s, avg total real prompt throughput: 1067.5 tokens/s, avg total final prompt throughput: 1067.5 tokens/s, avg total generation throughput: 1118.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Waiting: 2 reqs, GPU KV cache usage: 95.1%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[1023], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1024], num_prompts=1, max_context_len=None,)
INFO 12-28 15:52:40 llm_engine.py:693] Avg past 5 seconds prompt throughput: 211.9 tokens/s, Avg past 5 seconds real prompt throughput: 211.9 tokens/s, Avg past 5 seconds generation throughput: 444.5 tokens/s, total prompt throughput: 233355.0 tokens/s, total real prompt throughput: 232070.0 tokens/s, total final prompt throughput: 232070.0 tokens/s, total generation throughput: 242889.0 tokens/s, avg total prompt throughput: 1058.3 tokens/s, avg total real prompt throughput: 1052.5 tokens/s, avg total final prompt throughput: 1052.5 tokens/s, avg total generation throughput: 1103.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 54.8%, CPU KV cache usage: 0.0%
INFO 12-28 15:52:45 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 176.6 tokens/s, total prompt throughput: 233355.0 tokens/s, total real prompt throughput: 232070.0 tokens/s, total final prompt throughput: 232070.0 tokens/s, total generation throughput: 243776.0 tokens/s, avg total prompt throughput: 1034.8 tokens/s, avg total real prompt throughput: 1029.1 tokens/s, avg total final prompt throughput: 1029.1 tokens/s, avg total generation throughput: 1082.7 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.1%, CPU KV cache usage: 0.0%
Throughput: 4.36 requests/s, 2086.26 tokens/s
PromptThroughput: 1016.89 tokens/s
OutputThroughput: 1069.37 tokens/s
Total Time : 229.22 s
