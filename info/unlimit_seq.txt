Namespace(backend='vllm', dataset='/home/jiangjz/llm/TDoppeladler/dataset/ShareGPT_V3_unfiltered_cleaned_split.json', dtype='auto', enforce_eager=False, hf_max_batch_size=None, input_len=None, max_model_len=None, model='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', n=1, num_prompts=1000, output_len=None, quantization=None, seed=0, tensor_parallel_size=1, tokenizer='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', trust_remote_code=False, use_beam_search=False)
ModelConfig hidden_size=4096, head_size=128, swap_space_bytes=32, num_kv_heads=32        num_layers=32
CacheConfig block_size=16, gpu_memory_utilization=0.9, swap_space_bytes=34359738368,         sliding_window=None
SchedulerConfig max_num_batched_tokens=4096, max_model_len=4096, max_num_seqs=256,         max_paddings=256
INFO 12-28 16:31:31 llm_engine.py:74] Initializing an LLM engine with config: model='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', tokenizer='/home/jiangjz/llm/TDoppeladler/model/vicuna-7b-v1.5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
input_metadata = InputMetadata(prompt_lens=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16], num_prompts=256, max_context_len=None,)
INFO 12-28 16:31:37 llm_engine.py:236] # GPU blocks: 954, # CPU blocks: 4096
INFO 12-28 16:31:42 model_runner.py:403] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-28 16:31:42 model_runner.py:407] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=4096,)
INFO 12-28 16:31:45 model_runner.py:449] Graph capturing finished in 2 secs.
promt select exit padding exceed_paddings=301, cur_paddings=210, exceeed_seq=11
input_metadata = InputMetadata(prompt_lens=[4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], num_prompts=91, max_context_len=None,)
promt select exit padding exceed_paddings=279, cur_paddings=182, exceeed_seq=16
input_metadata = InputMetadata(prompt_lens=[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15], num_prompts=97, max_context_len=None,)
promt select exit padding exceed_paddings=274, cur_paddings=202, exceeed_seq=22
input_metadata = InputMetadata(prompt_lens=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21], num_prompts=72, max_context_len=None,)
promt select exit padding exceed_paddings=272, cur_paddings=209, exceeed_seq=30
input_metadata = InputMetadata(prompt_lens=[22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29], num_prompts=63, max_context_len=None,)
promt select exit padding exceed_paddings=259, cur_paddings=208, exceeed_seq=39
input_metadata = InputMetadata(prompt_lens=[30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38], num_prompts=51, max_context_len=None,)
promt select exit padding exceed_paddings=282, cur_paddings=241, exceeed_seq=51
input_metadata = InputMetadata(prompt_lens=[39, 39, 39, 39, 40, 40, 40, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 44, 44, 44, 44, 44, 45, 45, 45, 46, 46, 47, 47, 47, 48, 49, 49, 49, 50, 50, 50, 50, 50], num_prompts=41, max_context_len=None,)
promt select exit padding exceed_paddings=288, cur_paddings=254, exceeed_seq=67
input_metadata = InputMetadata(prompt_lens=[51, 52, 53, 53, 54, 55, 55, 55, 56, 56, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 59, 59, 59, 61, 62, 62, 62, 63, 63, 64, 64, 66, 66, 66], num_prompts=34, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[67, 67, 68, 68, 72, 72, 73, 73, 74, 76, 77, 77, 78, 78, 79, 79, 79, 80, 80], num_prompts=19, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=81,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=82,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=83,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=84,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=85,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=86,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=87,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=88,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=89,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=90,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=91,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=92,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=93,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=94,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=95,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=96,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=97,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=98,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=99,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=100,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=101,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=102,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=103,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=104,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=105,)
input_metadata = InputMetadata(prompt_lens=[], num_prompts=0, max_context_len=106,)
INFO 12-28 16:31:50 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2593.1 tokens/s, Avg past 5 seconds real prompt throughput: 2265.9 tokens/s, Avg past 5 seconds generation throughput: 4740.7 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 15668.0 tokens/s, avg total prompt throughput: 2593.1 tokens/s, avg total real prompt throughput: 2265.9 tokens/s, avg total final prompt throughput: 2265.9 tokens/s, avg total generation throughput: 4740.7 tokens/s, Running: 166 reqs, Swapped: 264 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 20.7%
INFO 12-28 16:31:55 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 2931.7 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 30396.0 tokens/s, avg total prompt throughput: 1241.9 tokens/s, avg total real prompt throughput: 1085.2 tokens/s, avg total final prompt throughput: 1085.2 tokens/s, avg total generation throughput: 3656.5 tokens/s, Running: 62 reqs, Swapped: 343 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.1%, CPU KV cache usage: 37.2%
INFO 12-28 16:32:00 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1609.1 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 38460.0 tokens/s, avg total prompt throughput: 816.8 tokens/s, avg total real prompt throughput: 713.7 tokens/s, avg total final prompt throughput: 713.7 tokens/s, avg total generation throughput: 2888.4 tokens/s, Running: 46 reqs, Swapped: 320 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 30.5%
INFO 12-28 16:32:05 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1548.6 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 46240.0 tokens/s, avg total prompt throughput: 607.9 tokens/s, avg total real prompt throughput: 531.2 tokens/s, avg total final prompt throughput: 531.2 tokens/s, avg total generation throughput: 2521.5 tokens/s, Running: 48 reqs, Swapped: 281 reqs, Waiting: 532 reqs, GPU KV cache usage: 98.4%, CPU KV cache usage: 23.2%
INFO 12-28 16:32:10 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1440.2 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 53469.0 tokens/s, avg total prompt throughput: 484.2 tokens/s, avg total real prompt throughput: 423.1 tokens/s, avg total final prompt throughput: 423.1 tokens/s, avg total generation throughput: 2289.7 tokens/s, Running: 31 reqs, Swapped: 279 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 24.5%
INFO 12-28 16:32:15 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1049.0 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 58733.0 tokens/s, avg total prompt throughput: 402.4 tokens/s, avg total real prompt throughput: 351.6 tokens/s, avg total final prompt throughput: 351.6 tokens/s, avg total generation throughput: 2070.5 tokens/s, Running: 31 reqs, Swapped: 266 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 21.1%
INFO 12-28 16:32:20 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1308.3 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 65290.0 tokens/s, avg total prompt throughput: 344.2 tokens/s, avg total real prompt throughput: 300.8 tokens/s, avg total final prompt throughput: 300.8 tokens/s, avg total generation throughput: 1955.9 tokens/s, Running: 38 reqs, Swapped: 239 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 17.7%
INFO 12-28 16:32:25 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1321.4 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 71931.0 tokens/s, avg total prompt throughput: 300.6 tokens/s, avg total real prompt throughput: 262.7 tokens/s, avg total final prompt throughput: 262.7 tokens/s, avg total generation throughput: 1872.8 tokens/s, Running: 43 reqs, Swapped: 212 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 14.5%
INFO 12-28 16:32:30 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1749.4 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 80682.0 tokens/s, avg total prompt throughput: 266.9 tokens/s, avg total real prompt throughput: 233.3 tokens/s, avg total final prompt throughput: 233.3 tokens/s, avg total generation throughput: 1858.3 tokens/s, Running: 46 reqs, Swapped: 182 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.2%, CPU KV cache usage: 12.0%
INFO 12-28 16:32:35 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1550.0 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 88507.0 tokens/s, avg total prompt throughput: 239.9 tokens/s, avg total real prompt throughput: 209.6 tokens/s, avg total final prompt throughput: 209.6 tokens/s, avg total generation throughput: 1826.2 tokens/s, Running: 74 reqs, Swapped: 129 reqs, Waiting: 532 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 7.6%
INFO 12-28 16:32:40 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1781.3 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 97452.0 tokens/s, avg total prompt throughput: 217.9 tokens/s, avg total real prompt throughput: 190.4 tokens/s, avg total final prompt throughput: 190.4 tokens/s, avg total generation throughput: 1822.4 tokens/s, Running: 53 reqs, Swapped: 120 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.6%, CPU KV cache usage: 6.9%
INFO 12-28 16:32:45 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1747.2 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 106223.0 tokens/s, avg total prompt throughput: 199.6 tokens/s, avg total real prompt throughput: 174.4 tokens/s, avg total final prompt throughput: 174.4 tokens/s, avg total generation throughput: 1815.9 tokens/s, Running: 57 reqs, Swapped: 85 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.7%, CPU KV cache usage: 4.8%
INFO 12-28 16:32:50 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1955.7 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 116052.0 tokens/s, avg total prompt throughput: 184.1 tokens/s, avg total real prompt throughput: 160.9 tokens/s, avg total final prompt throughput: 160.9 tokens/s, avg total generation throughput: 1827.0 tokens/s, Running: 65 reqs, Swapped: 45 reqs, Waiting: 532 reqs, GPU KV cache usage: 99.8%, CPU KV cache usage: 2.6%
INFO 12-28 16:32:55 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 1644.4 tokens/s, total prompt throughput: 11936.0 tokens/s, total real prompt throughput: 10430.0 tokens/s, total final prompt throughput: 10430.0 tokens/s, total generation throughput: 124332.0 tokens/s, avg total prompt throughput: 170.9 tokens/s, avg total real prompt throughput: 149.3 tokens/s, avg total final prompt throughput: 149.3 tokens/s, avg total generation throughput: 1813.7 tokens/s, Running: 68 reqs, Swapped: 9 reqs, Waiting: 532 reqs, GPU KV cache usage: 98.8%, CPU KV cache usage: 0.4%
input_metadata = InputMetadata(prompt_lens=[81, 81, 82, 82], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[83, 83, 83, 84], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[84, 85, 85, 85], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[86, 87, 87], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[88, 88], num_prompts=2, max_context_len=None,)
INFO 12-28 16:33:00 llm_engine.py:693] Avg past 5 seconds prompt throughput: 448.5 tokens/s, Avg past 5 seconds real prompt throughput: 446.0 tokens/s, Avg past 5 seconds generation throughput: 1781.4 tokens/s, total prompt throughput: 14721.0 tokens/s, total real prompt throughput: 13105.0 tokens/s, total final prompt throughput: 13105.0 tokens/s, total generation throughput: 133325.0 tokens/s, avg total prompt throughput: 196.6 tokens/s, avg total real prompt throughput: 175.0 tokens/s, avg total final prompt throughput: 175.0 tokens/s, avg total generation throughput: 1811.9 tokens/s, Running: 51 reqs, Swapped: 11 reqs, Waiting: 515 reqs, GPU KV cache usage: 99.1%, CPU KV cache usage: 1.4%
input_metadata = InputMetadata(prompt_lens=[88], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[89, 89], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[89], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[89], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[90, 90], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[92, 92, 92, 93, 94], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[94, 95, 95, 96, 96, 97], num_prompts=6, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[97, 99, 99, 99], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[99], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[100], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[101, 101, 101, 101, 103], num_prompts=5, max_context_len=None,)
INFO 12-28 16:33:05 llm_engine.py:693] Avg past 5 seconds prompt throughput: 589.6 tokens/s, Avg past 5 seconds real prompt throughput: 584.9 tokens/s, Avg past 5 seconds generation throughput: 1630.3 tokens/s, total prompt throughput: 17168.0 tokens/s, total real prompt throughput: 15534.0 tokens/s, total final prompt throughput: 15534.0 tokens/s, total generation throughput: 141502.0 tokens/s, avg total prompt throughput: 214.9 tokens/s, avg total real prompt throughput: 194.4 tokens/s, avg total final prompt throughput: 194.4 tokens/s, avg total generation throughput: 1800.4 tokens/s, Running: 55 reqs, Swapped: 1 reqs, Waiting: 486 reqs, GPU KV cache usage: 99.5%, CPU KV cache usage: 0.1%
input_metadata = InputMetadata(prompt_lens=[104, 105], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[106], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[107, 107, 108, 108, 109], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[109, 112, 112, 114, 115, 118, 119, 121], num_prompts=8, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[121], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[121, 122], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[123, 123, 124], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[126], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[126], num_prompts=1, max_context_len=None,)
INFO 12-28 16:33:10 llm_engine.py:693] Avg past 5 seconds prompt throughput: 602.8 tokens/s, Avg past 5 seconds real prompt throughput: 589.8 tokens/s, Avg past 5 seconds generation throughput: 1593.6 tokens/s, total prompt throughput: 20375.0 tokens/s, total real prompt throughput: 18675.0 tokens/s, total final prompt throughput: 18675.0 tokens/s, total generation throughput: 149487.0 tokens/s, avg total prompt throughput: 240.0 tokens/s, avg total real prompt throughput: 220.0 tokens/s, avg total final prompt throughput: 220.0 tokens/s, avg total generation throughput: 1788.1 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Waiting: 462 reqs, GPU KV cache usage: 99.4%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[131], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[131, 131, 131, 131], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[132], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[133, 134], num_prompts=2, max_context_len=None,)
INFO 12-28 16:33:15 llm_engine.py:693] Avg past 5 seconds prompt throughput: 169.5 tokens/s, Avg past 5 seconds real prompt throughput: 169.5 tokens/s, Avg past 5 seconds generation throughput: 1275.5 tokens/s, total prompt throughput: 21288.0 tokens/s, total real prompt throughput: 19588.0 tokens/s, total final prompt throughput: 19588.0 tokens/s, total generation throughput: 155904.0 tokens/s, avg total prompt throughput: 236.7 tokens/s, avg total real prompt throughput: 217.8 tokens/s, avg total final prompt throughput: 217.8 tokens/s, avg total generation throughput: 1759.1 tokens/s, Running: 32 reqs, Swapped: 6 reqs, Waiting: 454 reqs, GPU KV cache usage: 94.7%, CPU KV cache usage: 3.1%
input_metadata = InputMetadata(prompt_lens=[135, 136, 136], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[137, 137], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[138, 138, 140, 142], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[143, 144, 146, 147, 148], num_prompts=5, max_context_len=None,)
INFO 12-28 16:33:20 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1683.4 tokens/s, Avg past 5 seconds real prompt throughput: 1668.6 tokens/s, Avg past 5 seconds generation throughput: 1035.2 tokens/s, total prompt throughput: 22806.0 tokens/s, total real prompt throughput: 21094.0 tokens/s, total final prompt throughput: 21094.0 tokens/s, total generation throughput: 161091.0 tokens/s, avg total prompt throughput: 240.2 tokens/s, avg total real prompt throughput: 222.2 tokens/s, avg total final prompt throughput: 222.2 tokens/s, avg total generation throughput: 1720.2 tokens/s, Running: 35 reqs, Swapped: 1 reqs, Waiting: 440 reqs, GPU KV cache usage: 98.4%, CPU KV cache usage: 0.4%
input_metadata = InputMetadata(prompt_lens=[148, 149], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[150, 151, 152, 156], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[159, 159, 164], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[164, 164, 166, 167], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[167], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[167, 167], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[167, 167, 167], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[168, 168, 168, 171], num_prompts=4, max_context_len=None,)
INFO 12-28 16:33:25 llm_engine.py:693] Avg past 5 seconds prompt throughput: 945.8 tokens/s, Avg past 5 seconds real prompt throughput: 935.7 tokens/s, Avg past 5 seconds generation throughput: 1085.2 tokens/s, total prompt throughput: 26630.0 tokens/s, total real prompt throughput: 24873.0 tokens/s, total final prompt throughput: 24873.0 tokens/s, total generation throughput: 166543.0 tokens/s, avg total prompt throughput: 266.4 tokens/s, avg total real prompt throughput: 248.8 tokens/s, avg total final prompt throughput: 248.8 tokens/s, avg total generation throughput: 1687.9 tokens/s, Running: 38 reqs, Swapped: 2 reqs, Waiting: 417 reqs, GPU KV cache usage: 97.8%, CPU KV cache usage: 0.8%
input_metadata = InputMetadata(prompt_lens=[171, 175, 175], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[175], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[178, 178, 179], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[180, 180], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[181], num_prompts=1, max_context_len=None,)
INFO 12-28 16:33:30 llm_engine.py:693] Avg past 5 seconds prompt throughput: 345.5 tokens/s, Avg past 5 seconds real prompt throughput: 344.3 tokens/s, Avg past 5 seconds generation throughput: 1165.8 tokens/s, total prompt throughput: 28911.0 tokens/s, total real prompt throughput: 27139.0 tokens/s, total final prompt throughput: 27139.0 tokens/s, total generation throughput: 172384.0 tokens/s, avg total prompt throughput: 275.4 tokens/s, avg total real prompt throughput: 258.5 tokens/s, avg total final prompt throughput: 258.5 tokens/s, avg total generation throughput: 1662.6 tokens/s, Running: 29 reqs, Swapped: 3 reqs, Waiting: 407 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 1.9%
input_metadata = InputMetadata(prompt_lens=[182, 183, 184], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[184, 185], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[187, 189, 189, 191, 192], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[194, 194], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[194], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[197], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[197, 201], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[202, 204], num_prompts=2, max_context_len=None,)
INFO 12-28 16:33:35 llm_engine.py:693] Avg past 5 seconds prompt throughput: 900.3 tokens/s, Avg past 5 seconds real prompt throughput: 894.5 tokens/s, Avg past 5 seconds generation throughput: 1056.4 tokens/s, total prompt throughput: 32155.0 tokens/s, total real prompt throughput: 30363.0 tokens/s, total final prompt throughput: 30363.0 tokens/s, total generation throughput: 177676.0 tokens/s, avg total prompt throughput: 292.4 tokens/s, avg total real prompt throughput: 276.1 tokens/s, avg total final prompt throughput: 276.1 tokens/s, avg total generation throughput: 1634.7 tokens/s, Running: 30 reqs, Swapped: 2 reqs, Waiting: 389 reqs, GPU KV cache usage: 94.4%, CPU KV cache usage: 0.8%
input_metadata = InputMetadata(prompt_lens=[206], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[209, 210], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[210], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[210, 211], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[211, 218, 219], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[220], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[223], num_prompts=1, max_context_len=None,)
INFO 12-28 16:33:40 llm_engine.py:693] Avg past 5 seconds prompt throughput: 457.2 tokens/s, Avg past 5 seconds real prompt throughput: 454.8 tokens/s, Avg past 5 seconds generation throughput: 1004.9 tokens/s, total prompt throughput: 34698.0 tokens/s, total real prompt throughput: 32893.0 tokens/s, total final prompt throughput: 32893.0 tokens/s, total generation throughput: 182701.0 tokens/s, avg total prompt throughput: 301.7 tokens/s, avg total real prompt throughput: 286.0 tokens/s, avg total final prompt throughput: 286.0 tokens/s, avg total generation throughput: 1606.9 tokens/s, Running: 27 reqs, Swapped: 1 reqs, Waiting: 378 reqs, GPU KV cache usage: 98.0%, CPU KV cache usage: 0.6%
input_metadata = InputMetadata(prompt_lens=[224], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[226, 228], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[228], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[232], num_prompts=1, max_context_len=None,)
INFO 12-28 16:33:45 llm_engine.py:693] Avg past 5 seconds prompt throughput: 620.3 tokens/s, Avg past 5 seconds real prompt throughput: 618.9 tokens/s, Avg past 5 seconds generation throughput: 878.8 tokens/s, total prompt throughput: 35829.0 tokens/s, total real prompt throughput: 34022.0 tokens/s, total final prompt throughput: 34022.0 tokens/s, total generation throughput: 187113.0 tokens/s, avg total prompt throughput: 298.5 tokens/s, avg total real prompt throughput: 283.5 tokens/s, avg total final prompt throughput: 283.5 tokens/s, avg total generation throughput: 1576.1 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Waiting: 373 reqs, GPU KV cache usage: 99.9%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[232, 232], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[233, 234, 234, 235], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[236, 236, 240, 243], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[243, 244], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[245], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[245], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[245], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[248], num_prompts=1, max_context_len=None,)
INFO 12-28 16:33:50 llm_engine.py:693] Avg past 5 seconds prompt throughput: 977.3 tokens/s, Avg past 5 seconds real prompt throughput: 971.3 tokens/s, Avg past 5 seconds generation throughput: 794.5 tokens/s, total prompt throughput: 39660.0 tokens/s, total real prompt throughput: 37831.0 tokens/s, total final prompt throughput: 37831.0 tokens/s, total generation throughput: 191100.0 tokens/s, avg total prompt throughput: 317.2 tokens/s, avg total real prompt throughput: 302.6 tokens/s, avg total final prompt throughput: 302.6 tokens/s, avg total generation throughput: 1544.5 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Waiting: 357 reqs, GPU KV cache usage: 98.6%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[248, 249, 249, 253], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[255], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[256], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[259], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[262, 262, 263, 264], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[269, 271, 274], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[276], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[277], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[278], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[282], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[282], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[283], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[285, 286], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[288], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[291], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[294], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[301], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[301], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[301, 302, 303], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[304], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[304], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[306], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[307, 308], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[308], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[309], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[310], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[314], num_prompts=1, max_context_len=None,)
INFO 12-28 16:33:55 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2482.0 tokens/s, Avg past 5 seconds real prompt throughput: 2475.5 tokens/s, Avg past 5 seconds generation throughput: 753.5 tokens/s, total prompt throughput: 51899.0 tokens/s, total real prompt throughput: 50039.0 tokens/s, total final prompt throughput: 50039.0 tokens/s, total generation throughput: 194873.0 tokens/s, avg total prompt throughput: 399.1 tokens/s, avg total real prompt throughput: 384.8 tokens/s, avg total final prompt throughput: 384.8 tokens/s, avg total generation throughput: 1513.8 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Waiting: 314 reqs, GPU KV cache usage: 97.8%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[314], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[316], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[320], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[320], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[322, 323], num_prompts=2, max_context_len=None,)
INFO 12-28 16:34:00 llm_engine.py:693] Avg past 5 seconds prompt throughput: 262.1 tokens/s, Avg past 5 seconds real prompt throughput: 262.1 tokens/s, Avg past 5 seconds generation throughput: 838.7 tokens/s, total prompt throughput: 53483.0 tokens/s, total real prompt throughput: 51623.0 tokens/s, total final prompt throughput: 51623.0 tokens/s, total generation throughput: 199088.0 tokens/s, avg total prompt throughput: 396.0 tokens/s, avg total real prompt throughput: 382.3 tokens/s, avg total final prompt throughput: 382.3 tokens/s, avg total generation throughput: 1488.5 tokens/s, Running: 21 reqs, Swapped: 1 reqs, Waiting: 308 reqs, GPU KV cache usage: 98.0%, CPU KV cache usage: 0.7%
input_metadata = InputMetadata(prompt_lens=[323, 325, 325, 328, 329], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[329, 329], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[330], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[331], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[332], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[333, 337], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[338, 338], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[341, 342], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[342], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[344, 344], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[348], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[348], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[349], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[350], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[352], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[353], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[355], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[356, 358, 358, 360], num_prompts=4, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[364], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[365], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[366, 366], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[367], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[369], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[370], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[373, 373], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[375], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[376], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[377], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[378], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:05 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3177.9 tokens/s, Avg past 5 seconds real prompt throughput: 3171.8 tokens/s, Avg past 5 seconds generation throughput: 637.7 tokens/s, total prompt throughput: 68825.0 tokens/s, total real prompt throughput: 66936.0 tokens/s, total final prompt throughput: 66936.0 tokens/s, total generation throughput: 202294.0 tokens/s, avg total prompt throughput: 491.4 tokens/s, avg total real prompt throughput: 477.9 tokens/s, avg total final prompt throughput: 477.9 tokens/s, avg total generation throughput: 1457.8 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Waiting: 265 reqs, GPU KV cache usage: 98.4%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[380], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[381], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[382], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[382, 383], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[384], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[385, 387], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[387, 387, 390, 390, 391, 392], num_prompts=6, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[397], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[397], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[397], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[399, 404], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[406], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[408], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[408], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[410], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[413, 413, 413], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[415], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[415], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[415], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[418], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:10 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2513.6 tokens/s, Avg past 5 seconds real prompt throughput: 2508.6 tokens/s, Avg past 5 seconds generation throughput: 713.0 tokens/s, total prompt throughput: 80737.0 tokens/s, total real prompt throughput: 78825.0 tokens/s, total final prompt throughput: 78825.0 tokens/s, total generation throughput: 205879.0 tokens/s, avg total prompt throughput: 556.5 tokens/s, avg total real prompt throughput: 543.3 tokens/s, avg total final prompt throughput: 543.3 tokens/s, avg total generation throughput: 1431.8 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Waiting: 235 reqs, GPU KV cache usage: 98.8%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[419], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[421], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[421], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[422], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[423], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[424], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[425], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[427], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[432], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[435, 438], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[438], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[439], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[439], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[441], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:15 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1528.2 tokens/s, Avg past 5 seconds real prompt throughput: 1527.5 tokens/s, Avg past 5 seconds generation throughput: 723.8 tokens/s, total prompt throughput: 87161.0 tokens/s, total real prompt throughput: 85246.0 tokens/s, total final prompt throughput: 85246.0 tokens/s, total generation throughput: 209514.0 tokens/s, avg total prompt throughput: 580.7 tokens/s, avg total real prompt throughput: 567.9 tokens/s, avg total final prompt throughput: 567.9 tokens/s, avg total generation throughput: 1408.0 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Waiting: 220 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[443], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[443, 444], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[445, 445], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[445], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[445, 445], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[447, 451], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[451, 452], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[452], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[457], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[457], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[458], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[458], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:20 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1562.0 tokens/s, Avg past 5 seconds real prompt throughput: 1560.8 tokens/s, Avg past 5 seconds generation throughput: 596.6 tokens/s, total prompt throughput: 95231.0 tokens/s, total real prompt throughput: 93310.0 tokens/s, total final prompt throughput: 93310.0 tokens/s, total generation throughput: 212514.0 tokens/s, avg total prompt throughput: 613.9 tokens/s, avg total real prompt throughput: 601.5 tokens/s, avg total final prompt throughput: 601.5 tokens/s, avg total generation throughput: 1381.5 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Waiting: 202 reqs, GPU KV cache usage: 98.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[458, 459], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[460, 460, 461], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[461, 461, 463, 464, 465], num_prompts=5, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[468], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[468], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[468], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[470], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[471], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[475], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[475, 476, 481], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[482], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[487], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[489, 490], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[493], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[496], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[498], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[498, 500], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[501], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[504], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[504], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[505], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[506], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[509], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[510, 512], num_prompts=2, max_context_len=None,)
INFO 12-28 16:34:25 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3350.2 tokens/s, Avg past 5 seconds real prompt throughput: 3344.5 tokens/s, Avg past 5 seconds generation throughput: 573.3 tokens/s, total prompt throughput: 112043.0 tokens/s, total real prompt throughput: 110094.0 tokens/s, total final prompt throughput: 110094.0 tokens/s, total generation throughput: 215399.0 tokens/s, avg total prompt throughput: 699.6 tokens/s, avg total real prompt throughput: 687.5 tokens/s, avg total final prompt throughput: 687.5 tokens/s, avg total generation throughput: 1356.0 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Waiting: 166 reqs, GPU KV cache usage: 96.1%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[517], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[518], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[519], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[520], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[521, 523, 523], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[525], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[529], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[532], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[534], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[536], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[536], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[538], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[539], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[540], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[541, 543], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[549, 550], num_prompts=2, max_context_len=None,)
INFO 12-28 16:34:30 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2181.2 tokens/s, Avg past 5 seconds real prompt throughput: 2180.4 tokens/s, Avg past 5 seconds generation throughput: 614.1 tokens/s, total prompt throughput: 123669.0 tokens/s, total real prompt throughput: 121714.0 tokens/s, total final prompt throughput: 121714.0 tokens/s, total generation throughput: 218492.0 tokens/s, avg total prompt throughput: 748.7 tokens/s, avg total real prompt throughput: 736.9 tokens/s, avg total final prompt throughput: 736.9 tokens/s, avg total generation throughput: 1333.3 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Waiting: 144 reqs, GPU KV cache usage: 96.0%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[551], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[554], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[554], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[558], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[559, 560], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[560, 563, 564], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[570, 578], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[580], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[580], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[584], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[585], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[587], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[593], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[596, 597], num_prompts=2, max_context_len=None,)
INFO 12-28 16:34:35 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2102.1 tokens/s, Avg past 5 seconds real prompt throughput: 2099.1 tokens/s, Avg past 5 seconds generation throughput: 571.2 tokens/s, total prompt throughput: 134463.0 tokens/s, total real prompt throughput: 132493.0 tokens/s, total final prompt throughput: 132493.0 tokens/s, total generation throughput: 221357.0 tokens/s, avg total prompt throughput: 790.1 tokens/s, avg total real prompt throughput: 778.6 tokens/s, avg total final prompt throughput: 778.6 tokens/s, avg total generation throughput: 1310.7 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Waiting: 125 reqs, GPU KV cache usage: 97.5%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[598], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[599], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[601], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[604], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[609, 609], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[611], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[612], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[613, 616], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[616], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[622], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[624], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:40 llm_engine.py:693] Avg past 5 seconds prompt throughput: 1485.3 tokens/s, Avg past 5 seconds real prompt throughput: 1484.7 tokens/s, Avg past 5 seconds generation throughput: 571.6 tokens/s, total prompt throughput: 142970.0 tokens/s, total real prompt throughput: 140996.0 tokens/s, total final prompt throughput: 140996.0 tokens/s, total generation throughput: 224226.0 tokens/s, avg total prompt throughput: 816.1 tokens/s, avg total real prompt throughput: 804.8 tokens/s, avg total final prompt throughput: 804.8 tokens/s, avg total generation throughput: 1289.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Waiting: 112 reqs, GPU KV cache usage: 97.3%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[627], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[627], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[629], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[633], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[637], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[652], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[656], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[658, 661], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[662], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[664, 665], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[668], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[668], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[672], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[672], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[673], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[673], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[675], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[678], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[682], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:45 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2712.7 tokens/s, Avg past 5 seconds real prompt throughput: 2711.9 tokens/s, Avg past 5 seconds generation throughput: 438.4 tokens/s, total prompt throughput: 156748.0 tokens/s, total real prompt throughput: 154770.0 tokens/s, total final prompt throughput: 154770.0 tokens/s, total generation throughput: 226430.0 tokens/s, avg total prompt throughput: 869.8 tokens/s, avg total real prompt throughput: 858.9 tokens/s, avg total final prompt throughput: 858.9 tokens/s, avg total generation throughput: 1265.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Waiting: 91 reqs, GPU KV cache usage: 94.7%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[684], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[686, 686], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[686], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[690], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[692], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[696], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[697], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[701], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[703], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[706], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[719], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[723], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[723], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[729], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:50 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2028.4 tokens/s, Avg past 5 seconds real prompt throughput: 2028.4 tokens/s, Avg past 5 seconds generation throughput: 493.2 tokens/s, total prompt throughput: 167222.0 tokens/s, total real prompt throughput: 165244.0 tokens/s, total final prompt throughput: 165244.0 tokens/s, total generation throughput: 228920.0 tokens/s, avg total prompt throughput: 902.7 tokens/s, avg total real prompt throughput: 892.1 tokens/s, avg total final prompt throughput: 892.1 tokens/s, avg total generation throughput: 1244.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Waiting: 76 reqs, GPU KV cache usage: 94.3%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[730, 733], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[734], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[746], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[752, 752], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[754], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[762], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[762], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[765], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[767], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[773], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[777], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[788], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[791], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[797, 802], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[804, 807], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[807], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[819], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[828], num_prompts=1, max_context_len=None,)
INFO 12-28 16:34:55 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3635.9 tokens/s, Avg past 5 seconds real prompt throughput: 3633.4 tokens/s, Avg past 5 seconds generation throughput: 399.9 tokens/s, total prompt throughput: 184184.0 tokens/s, total real prompt throughput: 182195.0 tokens/s, total final prompt throughput: 182195.0 tokens/s, total generation throughput: 230926.0 tokens/s, avg total prompt throughput: 968.2 tokens/s, avg total real prompt throughput: 957.7 tokens/s, avg total final prompt throughput: 957.7 tokens/s, avg total generation throughput: 1222.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Waiting: 54 reqs, GPU KV cache usage: 94.5%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[834], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[836], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[839], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[840], num_prompts=1, max_context_len=None,)
INFO 12-28 16:35:00 llm_engine.py:693] Avg past 5 seconds prompt throughput: 510.8 tokens/s, Avg past 5 seconds real prompt throughput: 510.8 tokens/s, Avg past 5 seconds generation throughput: 530.0 tokens/s, total prompt throughput: 187521.0 tokens/s, total real prompt throughput: 185532.0 tokens/s, total final prompt throughput: 185532.0 tokens/s, total generation throughput: 233590.0 tokens/s, avg total prompt throughput: 960.4 tokens/s, avg total real prompt throughput: 950.2 tokens/s, avg total final prompt throughput: 950.2 tokens/s, avg total generation throughput: 1204.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Waiting: 50 reqs, GPU KV cache usage: 98.1%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[846], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[848, 850], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[856], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[857], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[862], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[870], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[873], num_prompts=1, max_context_len=None,)
INFO 12-28 16:35:05 llm_engine.py:693] Avg past 5 seconds prompt throughput: 3465.9 tokens/s, Avg past 5 seconds real prompt throughput: 3464.8 tokens/s, Avg past 5 seconds generation throughput: 419.9 tokens/s, total prompt throughput: 194352.0 tokens/s, total real prompt throughput: 192361.0 tokens/s, total final prompt throughput: 192361.0 tokens/s, total generation throughput: 235695.0 tokens/s, avg total prompt throughput: 970.5 tokens/s, avg total real prompt throughput: 960.5 tokens/s, avg total final prompt throughput: 960.5 tokens/s, avg total generation throughput: 1184.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Waiting: 42 reqs, GPU KV cache usage: 96.9%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[874], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[879], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[883], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[883], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[886], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[892, 899], num_prompts=2, max_context_len=None,)
INFO 12-28 16:35:10 llm_engine.py:693] Avg past 5 seconds prompt throughput: 897.4 tokens/s, Avg past 5 seconds real prompt throughput: 897.4 tokens/s, Avg past 5 seconds generation throughput: 429.9 tokens/s, total prompt throughput: 199630.0 tokens/s, total real prompt throughput: 197639.0 tokens/s, total final prompt throughput: 197639.0 tokens/s, total generation throughput: 237868.0 tokens/s, avg total prompt throughput: 972.3 tokens/s, avg total real prompt throughput: 962.6 tokens/s, avg total final prompt throughput: 962.6 tokens/s, avg total generation throughput: 1165.9 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Waiting: 35 reqs, GPU KV cache usage: 94.9%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[900], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[906, 911, 913], num_prompts=3, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[915], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[917], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[917], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[918], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[925], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[927], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[933, 936], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[937], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[943, 943], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[944], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[952, 952], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[956], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[963], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[965], num_prompts=1, max_context_len=None,)
INFO 12-28 16:35:15 llm_engine.py:693] Avg past 5 seconds prompt throughput: 4037.9 tokens/s, Avg past 5 seconds real prompt throughput: 4035.3 tokens/s, Avg past 5 seconds generation throughput: 319.5 tokens/s, total prompt throughput: 220048.0 tokens/s, total real prompt throughput: 218038.0 tokens/s, total final prompt throughput: 218038.0 tokens/s, total generation throughput: 239475.0 tokens/s, avg total prompt throughput: 1046.2 tokens/s, avg total real prompt throughput: 1036.6 tokens/s, avg total final prompt throughput: 1036.6 tokens/s, avg total generation throughput: 1145.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Waiting: 14 reqs, GPU KV cache usage: 93.9%, CPU KV cache usage: 0.0%
input_metadata = InputMetadata(prompt_lens=[976], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[979, 992], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[999], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1003], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1006], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1007], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1010], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1016], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1017, 1017], num_prompts=2, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1022], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1023], num_prompts=1, max_context_len=None,)
input_metadata = InputMetadata(prompt_lens=[1024], num_prompts=1, max_context_len=None,)
INFO 12-28 16:35:20 llm_engine.py:693] Avg past 5 seconds prompt throughput: 2732.6 tokens/s, Avg past 5 seconds real prompt throughput: 2729.9 tokens/s, Avg past 5 seconds generation throughput: 389.6 tokens/s, total prompt throughput: 234093.0 tokens/s, total real prompt throughput: 232070.0 tokens/s, total final prompt throughput: 232070.0 tokens/s, total generation throughput: 241427.0 tokens/s, avg total prompt throughput: 1087.1 tokens/s, avg total real prompt throughput: 1077.7 tokens/s, avg total final prompt throughput: 1077.7 tokens/s, avg total generation throughput: 1128.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 93.2%, CPU KV cache usage: 0.0%
INFO 12-28 16:35:25 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 377.6 tokens/s, total prompt throughput: 234093.0 tokens/s, total real prompt throughput: 232070.0 tokens/s, total final prompt throughput: 232070.0 tokens/s, total generation throughput: 243322.0 tokens/s, avg total prompt throughput: 1062.4 tokens/s, avg total real prompt throughput: 1053.2 tokens/s, avg total final prompt throughput: 1053.2 tokens/s, avg total generation throughput: 1110.8 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.0%, CPU KV cache usage: 0.0%
INFO 12-28 16:35:30 llm_engine.py:693] Avg past 5 seconds prompt throughput: 0.0 tokens/s, Avg past 5 seconds real prompt throughput: 0.0 tokens/s, Avg past 5 seconds generation throughput: 126.9 tokens/s, total prompt throughput: 234093.0 tokens/s, total real prompt throughput: 232070.0 tokens/s, total final prompt throughput: 232070.0 tokens/s, total generation throughput: 243960.0 tokens/s, avg total prompt throughput: 1038.8 tokens/s, avg total real prompt throughput: 1029.8 tokens/s, avg total final prompt throughput: 1029.8 tokens/s, avg total generation throughput: 1088.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.3%, CPU KV cache usage: 0.0%
Throughput: 4.39 requests/s, 2101.56 tokens/s
PromptThroughput: 1024.35 tokens/s
OutputThroughput: 1077.22 tokens/s
Total Time : 227.55 s
